{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requiresment.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.54.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2025.7.34)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.34.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.9.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (9.5.1.17)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (2.26.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch>=2.1->transformers[torch]) (59.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2025.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECAPA - TDNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class SEModule(nn.Module):\n",
    "    def __init__(self, channels, bottleneck=128):\n",
    "        \"\"\"\n",
    "        Squeeze-Excitation Module for channel-wise attention\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input channels\n",
    "            bottleneck: Dimension of the bottleneck representation\n",
    "        \"\"\"\n",
    "        super(SEModule, self).__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(channels, bottleneck, kernel_size=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(bottleneck, channels, kernel_size=1, padding=0),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        se = self.se(x)\n",
    "        return x * se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Res2Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2Block(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=3, dilation=1, scale=8):\n",
    "        \"\"\"\n",
    "        Res2Net Block for multi-scale feature extraction\n",
    "        \n",
    "        Args:\n",
    "            channels: Number of input/output channels\n",
    "            kernel_size: Size of the convolutional kernel\n",
    "            dilation: Dilation rate for the convolutions\n",
    "            scale: Number of scales for feature extraction\n",
    "        \"\"\"\n",
    "        super(Res2Block, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.width = channels // scale\n",
    "        self.nums = scale if channels % scale == 0 else scale - 1\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(\n",
    "                nn.Conv1d(\n",
    "                    self.width,\n",
    "                    self.width,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                    padding=dilation * (kernel_size - 1) // 2,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        spx = torch.split(x, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            sp = self.relu(self.convs[i](sp))\n",
    "            out.append(sp)\n",
    "        if self.scale - self.nums > 0:\n",
    "            out.append(spx[self.nums])\n",
    "        out = torch.cat(out, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SE-Res2Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SERes2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, scale=8):\n",
    "        \"\"\"\n",
    "        SE-Res2Block: Combines Res2Net with Squeeze-Excitation\n",
    "        \n",
    "        Args:\n",
    "            in_channels: Number of input channels\n",
    "            out_channels: Number of output channels\n",
    "            kernel_size: Size of the convolutional kernel\n",
    "            dilation: Dilation rate for the convolutions\n",
    "            scale: Number of scales for Res2Net\n",
    "        \"\"\"\n",
    "        super(SERes2Block, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.res2block = Res2Block(out_channels, kernel_size, dilation, scale)\n",
    "        self.se = SEModule(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.res2block(x)\n",
    "        x = self.se(x)\n",
    "        x = self.conv2(x)\n",
    "        return x + residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attentive Stats Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentiveStatsPooling(nn.Module):\n",
    "    def __init__(self, in_dim, bottleneck_dim=128):\n",
    "        \"\"\"\n",
    "        Attentive Statistics Pooling\n",
    "        \n",
    "        Args:\n",
    "            in_dim: Number of input channels\n",
    "            bottleneck_dim: Dimension of the bottleneck in the attention mechanism\n",
    "        \"\"\"\n",
    "        super(AttentiveStatsPooling, self).__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Conv1d(in_dim, bottleneck_dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(bottleneck_dim, in_dim, kernel_size=1),\n",
    "            nn.Softmax(dim=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is (batch, channels, time)\n",
    "        attention_weights = self.attention(x)\n",
    "        \n",
    "        # Weighted mean\n",
    "        mean = torch.sum(x * attention_weights, dim=2)\n",
    "        \n",
    "        # Weighted standard deviation\n",
    "        var = torch.sum(x**2 * attention_weights, dim=2) - mean**2\n",
    "        std = torch.sqrt(var.clamp(min=1e-5))\n",
    "        \n",
    "        # Concatenate mean and standard deviation\n",
    "        pooled = torch.cat([mean, std], dim=1)\n",
    "        return pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECAPA_TDNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ECAPA_TDNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=80,         # Input feature dimension (e.g., 80 filterbank features)\n",
    "        channels=512,         # Number of channels in the convolutional blocks\n",
    "        embedding_dim=192,    # Final embedding dimension\n",
    "        res2net_scale=8,      # Scale parameter for Res2Net blocks\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ECAPA-TDNN model for speaker verification\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Dimension of the input features\n",
    "            channels: Number of channels in the convolutional blocks\n",
    "            embedding_dim: Dimension of the final speaker embedding\n",
    "            res2net_scale: Scale parameter for Res2Net blocks\n",
    "        \"\"\"\n",
    "        super(ECAPA_TDNN, self).__init__()\n",
    "        \n",
    "        # Initial Conv1D + ReLU + BN\n",
    "        self.conv1 = nn.Conv1d(input_dim, channels, kernel_size=5, padding=2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(channels)\n",
    "        \n",
    "        # Three SE-Res2Blocks with different dilations (2, 3, 4)\n",
    "        self.se_res2block1 = SERes2Block(\n",
    "            channels, channels, kernel_size=3, dilation=2, scale=res2net_scale\n",
    "        )\n",
    "        self.se_res2block2 = SERes2Block(\n",
    "            channels, channels, kernel_size=3, dilation=3, scale=res2net_scale\n",
    "        )\n",
    "        self.se_res2block3 = SERes2Block(\n",
    "            channels, channels, kernel_size=3, dilation=4, scale=res2net_scale\n",
    "        )\n",
    "        \n",
    "        # Multi-Layer Feature Aggregation + Conv1D\n",
    "        self.conv_agg = nn.Conv1d(3 * channels, 1536, kernel_size=1)\n",
    "        self.bn_agg = nn.BatchNorm1d(1536)\n",
    "        \n",
    "        # Attentive Statistics Pooling + BN\n",
    "        self.asp = AttentiveStatsPooling(1536)\n",
    "        self.bn_asp = nn.BatchNorm1d(3072)  # 1536*2 because we concatenate mean and std\n",
    "        \n",
    "        # Final FC layer + BN for embedding\n",
    "        self.fc = nn.Linear(3072, embedding_dim)\n",
    "        self.bn_emb = nn.BatchNorm1d(embedding_dim)\n",
    "\n",
    "    def forward(self, input_values=None, labels=None, attention_mask=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass of the ECAPA-TDNN model\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, time_steps, input_dim)\n",
    "               or (batch_size, input_dim, time_steps)\n",
    "        \n",
    "        Returns:\n",
    "            Speaker embedding of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Check input shape and transpose if needed\n",
    "        x = input_values\n",
    "        if x.size(1) == self.conv1.in_channels:\n",
    "            # Input is already in the format (batch, channels, time)\n",
    "            pass\n",
    "        else:\n",
    "            # Input is in the format (batch, time, channels)\n",
    "            x = x.transpose(1, 2)\n",
    "        \n",
    "        # Initial Conv1D + ReLU + BN\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        # Three SE-Res2Blocks\n",
    "        x1 = self.se_res2block1(x)\n",
    "        x2 = self.se_res2block2(x1)\n",
    "        x3 = self.se_res2block3(x2)\n",
    "        \n",
    "        # Multi-Layer Feature Aggregation + Conv1D + ReLU + BN\n",
    "        x = torch.cat([x1, x2, x3], dim=1)\n",
    "        x = self.conv_agg(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_agg(x)\n",
    "        \n",
    "        # Attentive Statistics Pooling + BN\n",
    "        x = self.asp(x)\n",
    "        x = self.bn_asp(x)\n",
    "        \n",
    "        # Final FC layer + BN for embedding\n",
    "        x = self.fc(x)\n",
    "        x = self.bn_emb(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AAMSoftmaxLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAMSoftmaxLoss(nn.Module):\n",
    "    def __init__(self, embedding_dim=192, num_speakers=815, margin=0.2, scale=30):\n",
    "        \"\"\"\n",
    "        Additive Angular Margin Softmax Loss\n",
    "        \n",
    "        Args:\n",
    "            embedding_dim: Dimension of the embeddings\n",
    "            num_speakers: Number of speakers in the training set\n",
    "            margin: Angular margin penalty\n",
    "            scale: Scale factor for the cosine values\n",
    "        \"\"\"\n",
    "        super(AAMSoftmaxLoss, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_speakers = num_speakers\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        \n",
    "        # Weight for the speaker classification\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_speakers, embedding_dim))\n",
    "        nn.init.xavier_normal_(self.weight, gain=1)\n",
    "        \n",
    "        # Pre-compute constants for efficiency\n",
    "        self.cos_m = math.cos(margin)\n",
    "        self.sin_m = math.sin(margin)\n",
    "        self.th = math.cos(math.pi - margin)\n",
    "        self.mm = math.sin(math.pi - margin) * margin\n",
    "        \n",
    "    def forward(self, embeddings, labels):\n",
    "        # Input validation\n",
    "        # Ensure labels are proper long integers and within range\n",
    "        labels = labels.long()\n",
    "        if torch.any(labels < 0) or torch.any(labels >= self.num_speakers):\n",
    "            raise ValueError(f\"Labels must be in range [0, {self.num_speakers-1}], got: min={labels.min().item()}, max={labels.max().item()}\")\n",
    "        \n",
    "        # Normalize embeddings and weights\n",
    "        embeddings_norm = F.normalize(embeddings, p=2, dim=1)\n",
    "        weight_norm = F.normalize(self.weight, p=2, dim=1)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        cosine = F.linear(embeddings_norm, weight_norm)\n",
    "        \n",
    "        # Add angular margin penalty\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        \n",
    "        # Apply one-hot encoding for the target labels\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, labels.view(-1, 1), 1)\n",
    "        \n",
    "        # Apply margin to the target classes only\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        \n",
    "        # Scale the output\n",
    "        output = output * self.scale\n",
    "        \n",
    "        # Cross entropy loss - ensure labels are in the expected format\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        \n",
    "        return loss, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets & Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.10/dist-packages (2.0.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mName: webrtcvad\n",
      "Version: 2.0.10\n",
      "Summary: Python interface to the Google WebRTC Voice Activity Detector (VAD)\n",
      "Home-page: https://github.com/wiseman/py-webrtcvad\n",
      "Author: John Wiseman\n",
      "Author-email: jjwiseman@gmail.com\n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: \n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir webrtcvad\n",
    "!pip show webrtcvad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "def remove_silence(waveform, sample_rate=16000, frame_duration_ms=30):\n",
    "    vad = webrtcvad.Vad(2)  # Moderate aggressiveness (0-3)\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "    waveform_int16 = (waveform_np * 32767).astype(np.int16)\n",
    "    frame_length = int(sample_rate * frame_duration_ms / 1000)\n",
    "    frames = [waveform_int16[i:i+frame_length] for i in range(0, len(waveform_int16), frame_length)]\n",
    "    \n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "        if len(frame) == frame_length and vad.is_speech(frame.tobytes(), sample_rate):\n",
    "            voiced_frames.append(frame)\n",
    "    \n",
    "    if voiced_frames:\n",
    "        voiced_waveform = np.concatenate(voiced_frames).astype(np.float32) / 32767\n",
    "        return torch.tensor(voiced_waveform, dtype=torch.float32).unsqueeze(0)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import librosa\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "class VSASV_Train(Dataset):\n",
    "    def __init__(self, train_path, root_dir, sr=16000, duration=3):\n",
    "        self.root_dir = root_dir\n",
    "        self.filepaths = []\n",
    "        self.labels = []\n",
    "        self.meta = []\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.max_length = sr * duration\n",
    "        self.mfbe_transform = T.MelSpectrogram(\n",
    "            sample_rate=16000,\n",
    "            n_fft=400,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            n_mels=80,\n",
    "            power=1.0  # dùng năng lượng\n",
    "        )\n",
    "        self.target_frames = int(self.duration * self.sr / 160)  # hop_length = 160\n",
    "\n",
    "        # Read file metadata\n",
    "        with open(train_path, 'r') as f:\n",
    "            for line in f:\n",
    "                speaker_id, path, _ = line.strip().split()\n",
    "                if \"spoof\" in path:\n",
    "                    continue\n",
    "                audio_path = os.path.join(self.root_dir, path)\n",
    "                if os.path.exists(audio_path):\n",
    "                    self.filepaths.append(audio_path)\n",
    "                    self.labels.append(speaker_id)\n",
    "                else:\n",
    "                    print(audio_path)\n",
    "\n",
    "        self.label_map = {label: idx for idx, label in enumerate(sorted(set(self.labels)))}\n",
    "        self.labels = [self.label_map[label] for label in self.labels]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the waveform\n",
    "        wav_path = self.filepaths[idx]\n",
    "        waveform, sample_rate = torchaudio.load(wav_path)\n",
    "        waveform = remove_silence(waveform)\n",
    "        \n",
    "        if sample_rate != self.sr:\n",
    "            resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.sr)\n",
    "            waveform = resampler(waveform)\n",
    "        #waveform = waveform[0]  # Convert [1, N] to [N]\n",
    "\n",
    "        mfbe = self.mfbe_transform(waveform).squeeze(0)\n",
    "        if mfbe.shape[-1] < self.target_frames:\n",
    "            mfbe = torch.nn.functional.pad(mfbe, (0, self.target_frames - mfbe.shape[-1]))\n",
    "        else:\n",
    "            mfbe = mfbe[:, :self.target_frames]        \n",
    "\n",
    "        # Get label \n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Return as dictionary to match the format expected by the trainer\n",
    "        return {\n",
    "            'input_values': mfbe,\n",
    "            'speaker_labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class DataCollatorVietnamCeleb:\n",
    "    def __call__(self, batch):\n",
    "        input_values = [item['input_values'] for item in batch]  # shape: [80, T]\n",
    "        labels = [item['speaker_labels'] for item in batch]  # adjust to your label format\n",
    "\n",
    "        # Pad MFCCs to the same time length (dim=2)\n",
    "        # Transpose to [T, 80] to pad along time dimension\n",
    "        input_values = [x.T for x in input_values]  # [T, 80]\n",
    "        input_padded = pad_sequence(input_values, batch_first=True)  # [B, T_max, 80]\n",
    "        input_padded = input_padded.transpose(1, 2)  # Back to [B, 80, T_max]\n",
    "\n",
    "        labels = torch.tensor(labels)  # or handle text/tokenized labels accordingly\n",
    "\n",
    "        return {\n",
    "            'input_values': input_padded,\n",
    "            'speaker_labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  DataLoader\n",
    "import torch\n",
    "train_path = 'train.txt'  # Folder lead to the Train Path\n",
    "root_dir = 'data'  # The folder to contain the audio file\n",
    "train_dataset = VSASV_Train(train_path, root_dir)\n",
    "# Data Collator\n",
    "train_collator = DataCollatorVietnamCeleb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292005"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "\n",
    "class VSASV_Validation(Dataset):\n",
    "    def __init__(self, val_path, root_dir, sr=16000, duration=10):\n",
    "        \"\"\"\n",
    "        Dataset for speaker verification validation using MFBE (Mel-Frequency Band Energies)\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.max_length = sr * duration\n",
    "\n",
    "        # MelSpectrogram to compute MFBE\n",
    "        self.mfbe_transform = T.MelSpectrogram(\n",
    "            sample_rate=self.sr,\n",
    "            n_fft=512,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            n_mels=80,\n",
    "            power=1.0  # Use energy instead of power^2 (which is default)\n",
    "        )\n",
    "\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Read val pairs\n",
    "        with open(val_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3:\n",
    "                    utt_path1, utt_path2, label = parts\n",
    "                    path1 = os.path.join(self.root_dir, utt_path1)\n",
    "                    path2 = os.path.join(self.root_dir, utt_path2)\n",
    "                    if os.path.exists(path1) and os.path.exists(path2):\n",
    "                        self.pairs.append((path1, path2))\n",
    "                        self.labels.append(int(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path1, path2 = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        mfbe1 = self._load_mfbe(path1)\n",
    "        mfbe2 = self._load_mfbe(path2)\n",
    "\n",
    "        return {\n",
    "            \"input_values\": mfbe1,\n",
    "            \"input_values2\": mfbe2,\n",
    "            \"pair_labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _load_mfbe(self, path):\n",
    "        waveform, sr = torchaudio.load(path)\n",
    "        if sr != self.sr:\n",
    "            waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=self.sr)\n",
    "        waveform = waveform[0]  # [1, T] → [T]\n",
    "        waveform = remove_silence(waveform)\n",
    "        mfbe = self.mfbe_transform(waveform).squeeze(0)  # [1, 80, T] → [80, T]\n",
    "        return mfbe\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator for Validation\n",
    "class ValidationDataCollator:\n",
    "    def __call__(self, batch):\n",
    "        input_values = [item['input_values'].squeeze(-1).T for item in batch]   # [T, 80]\n",
    "        input_values2 = [item['input_values2'].squeeze(-1).T for item in batch] # [T, 80]\n",
    "        pair_labels = torch.stack([item['pair_labels'] for item in batch])  # assume fixed-size\n",
    "\n",
    "        # Pad both input sets\n",
    "        input_values_padded = pad_sequence(input_values, batch_first=True)   # [B, T_max, 80]\n",
    "        input_values2_padded = pad_sequence(input_values2, batch_first=True) # [B, T_max, 80]\n",
    "\n",
    "        # Transpose back to [B, 80, T_max]\n",
    "        input_values_padded = input_values_padded.transpose(1, 2)\n",
    "        input_values2_padded = input_values2_padded.transpose(1, 2)\n",
    "\n",
    "        return {\n",
    "            'input_values': input_values_padded,\n",
    "            'input_values2': input_values2_padded,\n",
    "            'pair_labels': pair_labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Subset\n",
    "random.seed(42)\n",
    "# Validation data\n",
    "val_path = \"val_pairs.txt\"\n",
    "val_dataset = VSASV_Validation(val_path, root_dir)\n",
    "val_collator = ValidationDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_values: shape = torch.Size([8, 80, 400])\n",
      "input_values2: shape = torch.Size([8, 80, 400])\n",
      "pair_labels: shape = torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=val_collator)\n",
    "\n",
    "batch = next(iter(val_loader))\n",
    "for k, v in batch.items():\n",
    "    print(f\"{k}: shape = {v.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined Data Collator to put both train and val Collator in one class\n",
    "class CombinedDataCollator:\n",
    "    def __init__(self, train_collator, val_collator):\n",
    "        self.train_collator = train_collator\n",
    "        self.val_collator = val_collator\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Check if this is validation data by looking for input_values2\n",
    "        if isinstance(batch[0], dict) and 'input_values2' in batch[0]:\n",
    "            return self.val_collator(batch)\n",
    "        else:\n",
    "            return self.train_collator(batch)\n",
    "combined_collator = CombinedDataCollator(train_collator, val_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WanDB setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install safetensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "# Log model architecture to wandb\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ECAPA_TDNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhddat2k4\u001b[0m (\u001b[33mhddat2k4-uit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250805_074717-czc352a7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hddat2k4-uit/ecapa-tdnn1/runs/czc352a7' target=\"_blank\">ecapa-tdnn</a></strong> to <a href='https://wandb.ai/hddat2k4-uit/ecapa-tdnn1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hddat2k4-uit/ecapa-tdnn1' target=\"_blank\">https://wandb.ai/hddat2k4-uit/ecapa-tdnn1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hddat2k4-uit/ecapa-tdnn1/runs/czc352a7' target=\"_blank\">https://wandb.ai/hddat2k4-uit/ecapa-tdnn1/runs/czc352a7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "os.environ[\"WANDB_KEY\"] = \"4b8af864ea6d5ec9af172b42a4c40e4444e20cf7\"\n",
    "\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.login(key=os.getenv(\"WANDB_KEY\"))\n",
    "wandb.init(\n",
    "    project=\"ecapa-tdnn1\",\n",
    "    name=\"ecapa-tdnn\",\n",
    "    config={\n",
    "        \"learning_rate\": 3e-5,\n",
    "        \"architecture\": \"ecapa-tdnn\",\n",
    "        \"dataset\": \"vsasv\",\n",
    "        \"epochs\": 50,\n",
    "    }\n",
    ")\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "# Define batch size and other hyperparameters\n",
    " \n",
    "\n",
    "\n",
    "# Update wandb config with additional hyperparameters\n",
    "wandb.config.update({\n",
    "    \"batch_size\": 256,\n",
    "    \"total_epochs\": 50,\n",
    "    \"aam_margin\": 0.2,\n",
    "    \"aam_scale\": 30,\n",
    "    \"num_speakers\": 815\n",
    "})\n",
    "\n",
    "# Load the model and move to device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = ECAPA_HuBERT().to(device)\n",
    "# state_dict = load_file(\"/kaggle/input/hubert_ecapa_v1/transformers/default/2/model_v2.safetensors\")\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "# # Log model architecture to wandb\n",
    "# wandb.watch(model, log=\"all\")\n",
    "\n",
    "# # Define batch size and other hyperparameters\n",
    "# batch_size = 8  \n",
    "# total_epochs = 5\n",
    "\n",
    "# # Update wandb config with additional hyperparameters\n",
    "# wandb.config.update({\n",
    "#     \"batch_size\": batch_size,\n",
    "#     \"total_epochs\": total_epochs,\n",
    "#     \"aam_margin\": 0.2,\n",
    "#     \"aam_scale\": 30,\n",
    "#     \"num_speakers\": 1000\n",
    "# })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eer(fnr, fpr, scores=None):\n",
    "    \"\"\"\n",
    "    Compute Equal Error Rate (EER) from false negative and false positive rates.\n",
    "    Returns: (eer, threshold)\n",
    "    \"\"\"\n",
    "    # Make sure fnr and fpr are numpy arrays\n",
    "    fnr = np.array(fnr)\n",
    "    fpr = np.array(fpr)\n",
    "    \n",
    "    # In case arrays are empty\n",
    "    if len(fnr) == 0 or len(fpr) == 0:\n",
    "        print(\"WARNING: Empty FNR or FPR arrays\")\n",
    "        return 0.5, 0.0  # Return default values\n",
    "    \n",
    "    # Calculate difference between FNR and FPR\n",
    "    diff = fnr - fpr\n",
    "    \n",
    "    # Find where the difference changes sign\n",
    "    # If diff changes sign, find the crossing point\n",
    "    if np.any(diff >= 0) and np.any(diff <= 0):\n",
    "        # Find indices where diff changes sign\n",
    "        positive_indices = np.flatnonzero(diff >= 0)\n",
    "        negative_indices = np.flatnonzero(diff <= 0)\n",
    "        \n",
    "        if len(positive_indices) > 0 and len(negative_indices) > 0:\n",
    "            # Get the boundary indices\n",
    "            idx1 = positive_indices[0]\n",
    "            idx2 = negative_indices[-1]\n",
    "            \n",
    "            # Check if indices are out of bounds\n",
    "            if idx1 >= len(fnr) or idx2 >= len(fnr):\n",
    "                print(\"WARNING: Index out of bounds\")\n",
    "                # Find closest points\n",
    "                abs_diff = np.abs(fnr - fpr)\n",
    "                min_idx = np.argmin(abs_diff)\n",
    "                eer = (fnr[min_idx] + fpr[min_idx]) / 2\n",
    "                threshold = scores[min_idx] if scores is not None else min_idx\n",
    "                return eer, threshold\n",
    "            \n",
    "            # Linear interpolation to find the EER\n",
    "            if fnr[idx1] == fpr[idx1]:\n",
    "                # Exactly equal at this point\n",
    "                eer = fnr[idx1]\n",
    "                threshold = scores[idx1] if scores is not None else idx1\n",
    "            else:\n",
    "                # Interpolate between idx1 and idx2\n",
    "                x = [fpr[idx2], fpr[idx1]]\n",
    "                y = [fnr[idx2], fnr[idx1]]\n",
    "                eer = np.mean(y)  # Approximate EER\n",
    "                \n",
    "                # If scores are provided, interpolate threshold\n",
    "                if scores is not None and idx1 < len(scores) and idx2 < len(scores):\n",
    "                    threshold = (scores[idx1] + scores[idx2]) / 2\n",
    "                else:\n",
    "                    threshold = (idx1 + idx2) / 2\n",
    "        else:\n",
    "            # Fallback if indices are not found\n",
    "            abs_diff = np.abs(fnr - fpr)\n",
    "            min_idx = np.argmin(abs_diff)\n",
    "            eer = (fnr[min_idx] + fpr[min_idx]) / 2\n",
    "            threshold = scores[min_idx] if scores is not None else min_idx\n",
    "    else:\n",
    "        # Fallback if no sign change - find the closest points\n",
    "        abs_diff = np.abs(fnr - fpr)\n",
    "        min_idx = np.argmin(abs_diff)\n",
    "        eer = (fnr[min_idx] + fpr[min_idx]) / 2\n",
    "        threshold = scores[min_idx] if scores is not None else min_idx\n",
    "    \n",
    "    return eer, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_speaker_metrics(eval_pred):\n",
    "    \"\"\"Compute EER metrics for speaker verification.\"\"\"\n",
    "    # Extract embeddings and labels\n",
    "    embeddings1 = eval_pred.embeddings1\n",
    "    embeddings2 = eval_pred.embeddings2\n",
    "    pair_labels = eval_pred.labels\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarity_scores = np.array([\n",
    "        np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-10)\n",
    "        for e1, e2 in zip(embeddings1, embeddings2)\n",
    "    ])\n",
    "    \n",
    "    # Compute FPR and FNR\n",
    "    thresholds = np.sort(similarity_scores)\n",
    "    fpr = np.zeros(len(thresholds))\n",
    "    fnr = np.zeros(len(thresholds))\n",
    "    \n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        # Predictions based on threshold\n",
    "        pred = (similarity_scores >= threshold).astype(int)\n",
    "        \n",
    "        # True positives, false positives, true negatives, false negatives\n",
    "        tp = np.sum((pred == 1) & (pair_labels == 1))\n",
    "        fp = np.sum((pred == 1) & (pair_labels == 0))\n",
    "        tn = np.sum((pred == 0) & (pair_labels == 0))\n",
    "        fn = np.sum((pred == 0) & (pair_labels == 1))\n",
    "        \n",
    "        # FPR and FNR\n",
    "        fpr[i] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr[i] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    # Calculate EER\n",
    "    eer, threshold = compute_eer(fnr, fpr, similarity_scores)\n",
    "    result = {\n",
    "        \"eer\": eer,\n",
    "        \"eer_threshold\": threshold\n",
    "    }\n",
    "    # Log EER to wandb directly\n",
    "    wandb.log({\"eer\": eer})\n",
    "    \n",
    "    # Create and log DET curve to wandb\n",
    "    if len(fpr) > 10:  # Only log if we have enough points\n",
    "        \n",
    "        # Log histogram of similarity scores\n",
    "        try:\n",
    "            wandb.log({\n",
    "                \"similarity_scores\": wandb.Histogram(similarity_scores),\n",
    "                \"same_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 1]),\n",
    "                \"diff_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 0])\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging histograms: {e}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Custom Trainer implementation for speaker verification\n",
    "class SpeakerVerificationTrainer(Trainer):\n",
    "    def __init__(self, *args, total_epochs=10, margin=0.2, scale=30, num_speakers=815, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.total_epochs = total_epochs\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        self.num_speakers = num_speakers\n",
    "        embedding_dim = 192  # This gets the embedding dimension\n",
    "        print(embedding_dim)\n",
    "        \n",
    "        # Initialize AAMSoftmax criterion\n",
    "        self.criterion = AAMSoftmaxLoss(\n",
    "            embedding_dim=embedding_dim,  # Get embedding dim from model\n",
    "            num_speakers=num_speakers,\n",
    "            margin=margin,\n",
    "            scale=scale\n",
    "        ).to(self.args.device)\n",
    "        \n",
    "        # For storing embeddings during evaluation\n",
    "        self.pairs_embeddings1 = []\n",
    "        self.pairs_embeddings2 = []\n",
    "        self.pairs_labels = []\n",
    "        \n",
    "        # Log criterion parameters to wandb\n",
    "        wandb.config.update({\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"aam_margin\": margin,\n",
    "            \"aam_scale\": scale,\n",
    "            \"num_speakers\": num_speakers\n",
    "        })\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        \"\"\"Create a working dataloader for training\"\"\"\n",
    "        # Create a simple dataloader that we know works\n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.args.train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=4,  # Critical: use single process\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "    \n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        \"\"\"Create a working dataloader for evaluation\"\"\"\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=4,  # Critical: use single process\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "    def scheduling(self, total_training_epoch, current_epoch, threshold=0.3):\n",
    "        \"\"\"Calculate the alpha value for the current epoch.\"\"\"\n",
    "        if total_training_epoch <= 1:\n",
    "            return threshold\n",
    "        alpha = (current_epoch - 1) / (total_training_epoch - 1)\n",
    "        return min(max(alpha, threshold), 1 - threshold)\n",
    "\n",
    "    def training_step(self, model, inputs, num_items=None):\n",
    "        \"\"\"Override training step to update alpha parameter.\"\"\"\n",
    "        # Get current epoch as integer (HF stores fractional)\n",
    "        current_epoch = int(self.state.epoch) + 1\n",
    "        new_alpha = self.scheduling(self.total_epochs, current_epoch)\n",
    "     \n",
    "        # Safely update alpha depending on model wrapping\n",
    "        if hasattr(model, 'module') and hasattr(model.module, 'alpha'):\n",
    "            model.module.alpha = new_alpha\n",
    "            alpha_value = model.module.alpha\n",
    "        elif hasattr(model, 'alpha'):\n",
    "            model.alpha = new_alpha\n",
    "            alpha_value = model.alpha\n",
    "        else:\n",
    "            alpha_value = None  # fallback\n",
    "    \n",
    "        # Print alpha update only at logging steps\n",
    "        if alpha_value is not None and self.state.global_step % self.args.logging_steps == 0:\n",
    "            self.log({\"alpha\": new_alpha})\n",
    "            print(f\"🔁 Epoch {current_epoch}: Alpha set to {alpha_value:.4f}\")\n",
    "            \n",
    "            # Also log to wandb\n",
    "            wandb.log({\"alpha\": new_alpha})\n",
    "    \n",
    "        # Let Trainer handle rest (loss computation, backprop, etc.)\n",
    "        return super().training_step(model, inputs, num_items)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Compute AAMSoftmax loss for the speaker embeddings.\"\"\"\n",
    "        # Extract inputs\n",
    "        input_values = inputs.get('input_values')\n",
    "        labels = inputs.get('speaker_labels')\n",
    "    \n",
    "        device = next(model.parameters()).device\n",
    "        input_values = input_values.to(device)\n",
    "        labels = labels.to(device) if labels is not None else None\n",
    "        \n",
    "        # Handle evaluation inputs with pairs for EER computation\n",
    "        is_eval_with_pairs = False\n",
    "        if not model.training and inputs.get('input_values2') is not None:\n",
    "            is_eval_with_pairs = True\n",
    "            input_values2 = inputs.get('input_values2').to(device)\n",
    "            pair_labels = inputs.get('pair_labels').to(device)\n",
    "        \n",
    "        # Forward pass to get speaker embeddings\n",
    "        embeddings = model(input_values)\n",
    "        \n",
    "        # Handle evaluation with pairs for EER\n",
    "        if is_eval_with_pairs:\n",
    "            # Get embeddings for second utterance in pairs\n",
    "            embeddings2 = model(input_values2)\n",
    "            \n",
    "            # Store pairs for EER calculation\n",
    "            self.pairs_embeddings1.append(embeddings.detach().cpu())\n",
    "            self.pairs_embeddings2.append(embeddings2.detach().cpu())\n",
    "            self.pairs_labels.append(pair_labels.detach().cpu())\n",
    "        \n",
    "        # Use AAMSoftmax loss for training\n",
    "        if labels is not None:\n",
    "            loss, outputs = self.criterion(embeddings, labels)\n",
    "            \n",
    "            # Log loss to wandb during training\n",
    "            if self.model.training and self.state.global_step % self.args.logging_steps == 0:\n",
    "                wandb.log({\"train/aam_loss\": loss.item()})\n",
    "        else:\n",
    "            loss = None\n",
    "            outputs = None\n",
    "        torch.cuda.empty_cache()\n",
    "        if return_outputs:\n",
    "            return loss, {\"loss\": loss, \"logits\": outputs, \"embeddings\": embeddings}\n",
    "        else:\n",
    "            return loss\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        # Call the parent class method to get regular outputs\n",
    "        outputs = super().prediction_step(model, inputs, prediction_loss_only, ignore_keys)\n",
    "        \n",
    "        # During evaluation, collect embeddings for pairs\n",
    "        if not prediction_loss_only:\n",
    "            with torch.no_grad():\n",
    "                # Get embeddings from model (adjust based on your model's output structure)\n",
    "                device = next(model.parameters()).device\n",
    "                embeddings1 = model(inputs[\"input_values\"].to(device))\n",
    "                embeddings2 = model(inputs[\"input_values2\"].to(device))\n",
    "                \n",
    "                # Store embeddings and labels\n",
    "                self.pairs_embeddings1.append(embeddings1.detach().cpu())\n",
    "                self.pairs_embeddings2.append(embeddings2.detach().cpu())\n",
    "                self.pairs_labels.append(inputs[\"pair_labels\"].detach().cpu())\n",
    "        \n",
    "        return outputs    \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"Override evaluate to compute EER at the end of evaluation.\"\"\"\n",
    "        # Reset storage for pairs\n",
    "        self.pairs_embeddings1 = []\n",
    "        self.pairs_embeddings2 = []\n",
    "        self.pairs_labels = []\n",
    "        \n",
    "        # Run standard evaluation\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # Calculate EER if we have collected pairs\n",
    "        if len(self.pairs_embeddings1) > 0:\n",
    "            # Prepare data for compute metrics function\n",
    "            embeddings1 = torch.cat(self.pairs_embeddings1, dim=0).numpy()\n",
    "            embeddings2 = torch.cat(self.pairs_embeddings2, dim=0).numpy()\n",
    "            pair_labels = torch.cat(self.pairs_labels, dim=0).numpy()\n",
    "            \n",
    "            # Create a container class to hold the embeddings\n",
    "            class EmbeddingPairs:\n",
    "                def __init__(self, embeddings1, embeddings2, labels):\n",
    "                    self.embeddings1 = embeddings1\n",
    "                    self.embeddings2 = embeddings2\n",
    "                    self.labels = labels\n",
    "            \n",
    "            eval_pairs = EmbeddingPairs(embeddings1, embeddings2, pair_labels)\n",
    "            \n",
    "            # Compute EER metrics\n",
    "            eer_metrics = compute_speaker_metrics(eval_pairs)\n",
    "            \n",
    "            # Add EER metrics to the overall metrics\n",
    "            for key, value in eer_metrics.items():\n",
    "                if key not in metrics:\n",
    "                    metrics[f\"{metric_key_prefix}_{key}\"] = value\n",
    "            \n",
    "            # Log to wandb with correct prefix\n",
    "            wandb_metrics = {\n",
    "                f\"{metric_key_prefix}/{key}\": value \n",
    "                for key, value in metrics.items() \n",
    "                if key.startswith(metric_key_prefix)\n",
    "            }\n",
    "            wandb.log(wandb_metrics)\n",
    "            \n",
    "            print(f\"\\n{metric_key_prefix.capitalize()} EER: {metrics.get(f'{metric_key_prefix}_eer', 0):.4f}\")\n",
    "            \n",
    "            # Log embedding visualizations to wandb (t-SNE of random subset)\n",
    "            if len(embeddings1) > 100:\n",
    "                try:\n",
    "                    from sklearn.manifold import TSNE\n",
    "                    # Sample a subset for visualization (for efficiency)\n",
    "                    max_samples = min(500, len(embeddings1))\n",
    "                    indices = np.random.choice(len(embeddings1), max_samples, replace=False)\n",
    "                    \n",
    "                    # Apply t-SNE\n",
    "                    tsne = TSNE(n_components=2, random_state=42)\n",
    "                    embeddings_combined = np.vstack([embeddings1[indices], embeddings2[indices]])\n",
    "                    embeddings_2d = tsne.fit_transform(embeddings_combined)\n",
    "                    \n",
    "                    # Split back into two sets\n",
    "                    n_samples = len(indices)\n",
    "                    embeddings1_2d = embeddings_2d[:n_samples]\n",
    "                    embeddings2_2d = embeddings_2d[n_samples:]\n",
    "                    \n",
    "                    # Create scatter plot data\n",
    "                    data = []\n",
    "                    for i in range(n_samples):\n",
    "                        data.append([\n",
    "                            embeddings1_2d[i, 0], embeddings1_2d[i, 1], \n",
    "                            \"Embedding 1\", int(pair_labels[indices[i]])\n",
    "                        ])\n",
    "                        data.append([\n",
    "                            embeddings2_2d[i, 0], embeddings2_2d[i, 1], \n",
    "                            \"Embedding 2\", int(pair_labels[indices[i]])\n",
    "                        ])\n",
    "                    \n",
    "                    # Log to wandb\n",
    "                    wandb.log({\n",
    "                        f\"{metric_key_prefix}/embeddings_tsne\": wandb.Table(\n",
    "                            data=data,\n",
    "                            columns=[\"x\", \"y\", \"embedding_type\", \"same_speaker\"]\n",
    "                        )\n",
    "                    })\n",
    "                except ImportError:\n",
    "                    print(\"sklearn not available for t-SNE visualization\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating t-SNE visualization: {e}\")\n",
    "        \n",
    "        return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoint1\",\n",
    "    per_device_train_batch_size=256,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=500,\n",
    "    per_device_eval_batch_size=256,\n",
    "    learning_rate=3e-5,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=51,\n",
    "    num_train_epochs=50,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=[\"wandb\"],  # Enable logging to wandb\n",
    "    metric_for_best_model=\"eval_eer\",\n",
    "    greater_is_better=False,  # Lower EER is better\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5574' max='57000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5574/57000 1:35:34 < 14:42:02, 0.97 it/s, Epoch 4.89/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.997400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.286200</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.281500</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.353600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval EER: 0.2149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval EER: 0.2280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval EER: 0.2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eval EER: 0.2123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n",
      "/tmp/ipykernel_3884/3207403718.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_values = [torch.tensor(item['input_values']) for item in batch]  # shape: [80, T]\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainer\n",
    "trainer = SpeakerVerificationTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset,    \n",
    "    data_collator=combined_collator,  \n",
    "    compute_metrics=compute_speaker_metrics,\n",
    "    total_epochs=int(training_args.num_train_epochs),\n",
    "    num_speakers=815,  # Adjust based on your dataset\n",
    "    margin=0.2,\n",
    "    scale=30\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# After training completes, save and log the best model to wandb\n",
    "trainer.save_model(training_args.output_dir + \"/best_model\")\n",
    "wandb.save(training_args.output_dir + \"/best_model/*\")\n",
    "    \n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7896850,
     "sourceId": 12511172,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7960721,
     "sourceId": 12603262,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 394068,
     "modelInstanceId": 373223,
     "sourceId": 461066,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
