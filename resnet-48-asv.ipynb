{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adbabcee-88c1-4e6e-85fd-0dc06d2acc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.7.1)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (2.7.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.11.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (2.2.6)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (0.13.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (2.32.4)\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (5.2.0)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.10.5)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.3.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (0.6.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (2.26.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.80)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (9.5.1.17)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (4.14.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 1)) (11.7.1.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch->-r requirements.txt (line 1)) (59.6.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (0.61.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (0.5.0.post1)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (1.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (3.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (5.2.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (1.5.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (0.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (1.15.3)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa->-r requirements.txt (line 3)) (1.8.2)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->-r requirements.txt (line 5)) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 7)) (2025.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 7)) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 7)) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 7)) (2.5.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 8)) (4.13.4)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (6.0.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (2.11.7)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (3.1.45)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (2.34.1)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (4.3.8)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (25.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 9)) (8.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 10)) (3.6.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (4.59.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (0.12.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib->-r requirements.txt (line 11)) (2.4.7)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (11.3.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (1.3.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 11)) (1.4.8)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf->-r requirements.txt (line 12)) (4.9.3)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 5)) (2.22)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (4.0.12)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 3)) (0.44.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (0.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb->-r requirements.txt (line 9)) (2.33.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 11)) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 8)) (2.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->-r requirements.txt (line 7)) (1.7.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 9)) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05adcf3b-092e-4577-bb48-0d02ba83d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.54.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2025.7.34)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.34.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.9.0)\n",
      "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (1.1.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (4.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers[torch]) (2025.7.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (11.7.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (0.6.3)\n",
      "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (1.11.1.6)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (12.6.77)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.1->torch>=2.1->transformers[torch]) (59.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2025.6.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1->transformers[torch]) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d81a4b9",
   "metadata": {
    "papermill": {
     "duration": 0.006111,
     "end_time": "2025-07-19T02:43:25.801868",
     "exception": false,
     "start_time": "2025-07-19T02:43:25.795757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ResNet 48 + MLCC + AAM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96522046",
   "metadata": {
    "papermill": {
     "duration": 0.006016,
     "end_time": "2025-07-19T02:43:25.814291",
     "exception": false,
     "start_time": "2025-07-19T02:43:25.808275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### BasicBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715a5013",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 5.353914,
     "end_time": "2025-07-19T02:43:31.174982",
     "exception": false,
     "start_time": "2025-07-19T02:43:25.821068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        if self.downsample:\n",
    "            identity = self.downsample(x)\n",
    "        out += identity\n",
    "        return self.relu(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022b6b4",
   "metadata": {
    "papermill": {
     "duration": 0.007149,
     "end_time": "2025-07-19T02:43:31.189226",
     "exception": false,
     "start_time": "2025-07-19T02:43:31.182077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### StatsPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0afb42-5d79-4508-ae56-f8ed7fe021c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsPooling(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, F, T]  (sau stage4: [B, 256, 10, T/8])\n",
    "        B, C, F, T = x.size()\n",
    "        x = x.view(B, C * F, T)         # [B, 2560, T/8]\n",
    "        mean = x.mean(dim=2)            # [B, 2560]\n",
    "        std = x.std(dim=2)              # [B, 2560]\n",
    "        return torch.cat([mean, std], dim=1)  # [B, 5120]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da855bc2-80d4-4c76-ab94-07d83c561f69",
   "metadata": {
    "papermill": {
     "duration": 0.007149,
     "end_time": "2025-07-19T02:43:31.189226",
     "exception": false,
     "start_time": "2025-07-19T02:43:31.182077",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### AAM Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6006ff3",
   "metadata": {
    "papermill": {
     "duration": 0.016846,
     "end_time": "2025-07-19T02:43:31.212578",
     "exception": false,
     "start_time": "2025-07-19T02:43:31.195732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AAMSoftmax(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, num_classes=1000, s=30.0, m=0.2):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, embedding_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x = F.normalize(x, dim=1)\n",
    "        W = F.normalize(self.weight, dim=1)\n",
    "        cosine = F.linear(x, W)  # [B, num_classes]\n",
    "        theta = torch.acos(torch.clamp(cosine, -1.0 + 1e-7, 1.0 - 1e-7))\n",
    "        target_logits = torch.cos(theta + self.m)\n",
    "        one_hot = F.one_hot(labels, num_classes=cosine.size(1)).float().to(x.device)\n",
    "        output = self.s * (one_hot * target_logits + (1.0 - one_hot) * cosine)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "        return loss, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0f006",
   "metadata": {
    "papermill": {
     "duration": 0.00608,
     "end_time": "2025-07-19T02:43:31.226004",
     "exception": false,
     "start_time": "2025-07-19T02:43:31.219924",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### ResNet-48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e1c9f84",
   "metadata": {
    "papermill": {
     "duration": 0.017986,
     "end_time": "2025-07-19T02:43:31.250221",
     "exception": false,
     "start_time": "2025-07-19T02:43:31.232235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResNet48_ASV(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, num_speakers=None):\n",
    "        super().__init__()\n",
    "        # Conv0\n",
    "        self.conv1 = nn.Conv2d(1, 96, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(96)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Residual stages\n",
    "        self.layer1 = self._make_layer(96, 96, 6, stride=1)    # ResBlock-1\n",
    "        self.layer2 = self._make_layer(96, 128, 8, stride=2)   # ResBlock-2\n",
    "        self.layer3 = self._make_layer(128, 160, 6, stride=2)  # ResBlock-3\n",
    "        self.layer4 = self._make_layer(160, 256, 3, stride=2)  # ResBlock-4\n",
    "\n",
    "        # Pooling + FC\n",
    "        self.pooling = StatsPooling()\n",
    "        self.fc = nn.Linear(5120, embedding_dim)\n",
    "\n",
    "        # Classifier (optional)\n",
    "        if num_speakers:\n",
    "            self.classifier = nn.Linear(embedding_dim, num_speakers)\n",
    "        else:\n",
    "            self.classifier = None\n",
    "\n",
    "    def _make_layer(self, in_c, out_c, num_blocks, stride):\n",
    "        layers = [BasicBlock(in_c, out_c, stride)]\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(BasicBlock(out_c, out_c))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_values, labels=None):\n",
    "        # Backbone\n",
    "        x = self.relu(self.bn1(self.conv1(input_values)))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        # StatsPooling\n",
    "        x = self.pooling(x)\n",
    "\n",
    "        # Dense → embedding\n",
    "        embeddings = F.normalize(self.fc(x), dim=1)  # [B, 256]\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cd5ce6",
   "metadata": {
    "papermill": {
     "duration": 0.006164,
     "end_time": "2025-07-19T02:43:31.262847",
     "exception": false,
     "start_time": "2025-07-19T02:43:31.256683",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train & Validation Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f82f3fe8",
   "metadata": {
    "papermill": {
     "duration": 10.466827,
     "end_time": "2025-07-19T02:43:41.735892",
     "exception": false,
     "start_time": "2025-07-19T02:43:31.269065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webrtcvad in /usr/local/lib/python3.10/dist-packages (2.0.10)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading webrtcvad-2.0.10.tar.gz (66 kB)\r\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/66.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.2/66.2 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Building wheels for collected packages: webrtcvad\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for webrtcvad (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for webrtcvad: filename=webrtcvad-2.0.10-cp311-cp311-linux_x86_64.whl size=73495 sha256=d53b693bef75f5a8e3dd786ed97de9d63f0eafb4eb29f26d2d5e22f5fce54738\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/94/65/3f/292d0b656be33d1c801831201c74b5f68f41a2ae465ff2ee2f\r\n",
      "Successfully built webrtcvad\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: webrtcvad\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed webrtcvad-2.0.10\r\n"
     ]
    }
   ],
   "source": [
    "!pip install webrtcvad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b7d9f31",
   "metadata": {
    "papermill": {
     "duration": 0.123295,
     "end_time": "2025-07-19T02:43:41.866573",
     "exception": false,
     "start_time": "2025-07-19T02:43:41.743278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "def remove_silence(waveform, sample_rate=16000, frame_duration_ms=30):\n",
    "    vad = webrtcvad.Vad(2)  # Moderate aggressiveness (0-3)\n",
    "    waveform_np = waveform.squeeze().numpy()\n",
    "    waveform_int16 = (waveform_np * 32767).astype(np.int16)\n",
    "    frame_length = int(sample_rate * frame_duration_ms / 1000)\n",
    "    frames = [waveform_int16[i:i+frame_length] for i in range(0, len(waveform_int16), frame_length)]\n",
    "    \n",
    "    voiced_frames = []\n",
    "    for frame in frames:\n",
    "        if len(frame) == frame_length and vad.is_speech(frame.tobytes(), sample_rate):\n",
    "            voiced_frames.append(frame)\n",
    "    \n",
    "    if voiced_frames:\n",
    "        voiced_waveform = np.concatenate(voiced_frames).astype(np.float32) / 32767\n",
    "        return torch.tensor(voiced_waveform, dtype=torch.float32).unsqueeze(0)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad19c1a",
   "metadata": {
    "papermill": {
     "duration": 0.007339,
     "end_time": "2025-07-19T02:43:41.881851",
     "exception": false,
     "start_time": "2025-07-19T02:43:41.874512",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train Data Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3eae8ac8",
   "metadata": {
    "papermill": {
     "duration": 1.058137,
     "end_time": "2025-07-19T02:43:42.947603",
     "exception": false,
     "start_time": "2025-07-19T02:43:41.889466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class VSASV_Train(Dataset):\n",
    "    def __init__(self, list_file, root_dir=\"/kaggle/input/vsasv-train/vlsp_train/home4/vuhl/VSASV-Dataset\",\n",
    "                 sample_rate=16000, max_duration=2, n_mels=80, n_fft=400, hop_length=160, f_min=20, f_max=None):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_samples = int(sample_rate * max_duration)\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        # MFBE transform (torchaudio MelSpectrogram)\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=sample_rate,\n",
    "            n_fft=n_fft,\n",
    "            hop_length=hop_length,\n",
    "            n_mels=n_mels,\n",
    "            f_min=f_min,\n",
    "            f_max=f_max,\n",
    "            power=2.0,  # power=2.0 để ra năng lượng\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "        # metadata\n",
    "        self.samples = []\n",
    "        self.speaker_map = {}\n",
    "        self.speaker_to_indices = {}\n",
    "        label_counter = 0\n",
    "\n",
    "        with open(list_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                spk, rel_path, label = line.strip().split()\n",
    "                if label != \"bonafide\":   # chỉ train trên bonafide\n",
    "                    continue\n",
    "\n",
    "                full_path = os.path.join(self.root_dir, rel_path)\n",
    "\n",
    "                if spk not in self.speaker_map:\n",
    "                    self.speaker_map[spk] = label_counter\n",
    "                    label_counter += 1\n",
    "\n",
    "                spk_id = self.speaker_map[spk]\n",
    "                self.samples.append((full_path, spk_id))\n",
    "\n",
    "                if spk_id not in self.speaker_to_indices:\n",
    "                    self.speaker_to_indices[spk_id] = []\n",
    "                self.speaker_to_indices[spk_id].append(len(self.samples) - 1)\n",
    "\n",
    "        self.speakers = list(self.speaker_to_indices.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        waveform, sr = torchaudio.load(path)   # [1, T]\n",
    "    \n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
    "\n",
    "        waveform = waveform.squeeze(0)  # [T]\n",
    "    \n",
    "        # Cắt ngẫu nhiên 2s\n",
    "        if waveform.shape[0] > self.max_samples:\n",
    "            max_start = waveform.shape[0] - self.max_samples\n",
    "            start = torch.randint(0, max_start + 1, (1,)).item()\n",
    "            waveform = waveform[start:start + self.max_samples]\n",
    "        else:\n",
    "            pad = self.max_samples - waveform.shape[0]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad))\n",
    "    \n",
    "        # MFBE transform\n",
    "        mel_spec = self.mel_transform(waveform)   # [n_mels, time]\n",
    "        mel_db = self.amplitude_to_db(mel_spec)   # log-scale\n",
    "\n",
    "        return {\n",
    "            \"input_values\": mel_db,  # [n_mels, time]\n",
    "            \"speaker_labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474f486",
   "metadata": {
    "papermill": {
     "duration": 0.006631,
     "end_time": "2025-07-19T02:43:42.961480",
     "exception": false,
     "start_time": "2025-07-19T02:43:42.954849",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97c37a8",
   "metadata": {
    "papermill": {
     "duration": 0.016757,
     "end_time": "2025-07-19T02:43:42.985141",
     "exception": false,
     "start_time": "2025-07-19T02:43:42.968384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class TrainDataCollator:\n",
    "    def __call__(self, batch):\n",
    "        # Giả sử input là [80, T] hoặc [80, T, 1], ta cần đưa về [T, 80]\n",
    "        input_values = [\n",
    "            item['input_values'].squeeze()  # loại bỏ chiều dư nếu có\n",
    "            .transpose(0, 1)                # [80, T] → [T, 80]\n",
    "            for item in batch\n",
    "        ]\n",
    "\n",
    "        # Pad theo T → [B, T_max, 80]\n",
    "        input_padded = pad_sequence(input_values, batch_first=True)\n",
    "\n",
    "        # Transpose lại về [B, 80, T_max]\n",
    "        input_padded = input_padded.transpose(1, 2)\n",
    "\n",
    "        # Thêm channel: [B, 1, 80, T_max]\n",
    "        input_padded = input_padded.unsqueeze(1)\n",
    "\n",
    "        labels = torch.tensor([item['speaker_labels'] for item in batch], dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_values': input_padded,      # ✅ [B, 1, 80, T]\n",
    "            'speaker_labels': labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "869c139d",
   "metadata": {
    "papermill": {
     "duration": 0.013424,
     "end_time": "2025-07-19T02:43:43.005312",
     "exception": false,
     "start_time": "2025-07-19T02:43:42.991888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_path = \"train_asv.txt\"\n",
    "root_dir = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0df39078",
   "metadata": {
    "papermill": {
     "duration": 0.36035,
     "end_time": "2025-07-19T02:43:43.372557",
     "exception": false,
     "start_time": "2025-07-19T02:43:43.012207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataset = VSASV_Train(\n",
    "    list_file=train_path,\n",
    "    root_dir=root_dir,\n",
    ")\n",
    "train_collator = TrainDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96acfd34",
   "metadata": {
    "papermill": {
     "duration": 0.006387,
     "end_time": "2025-07-19T02:43:43.386315",
     "exception": false,
     "start_time": "2025-07-19T02:43:43.379928",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "490cabbb",
   "metadata": {
    "papermill": {
     "duration": 0.022064,
     "end_time": "2025-07-19T02:43:43.415059",
     "exception": false,
     "start_time": "2025-07-19T02:43:43.392995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VSASV_Validation(Dataset):\n",
    "    def __init__(self, val_path, root_dir, sr=16000, duration=5.0):\n",
    "        \"\"\"\n",
    "        Dataset for speaker verification validation using pre-defined utterance pairs\n",
    "\n",
    "        Args:\n",
    "            val_path: Path to validation file with pre-defined pairs\n",
    "            root_dir: Root directory containing the audio files\n",
    "            sr: Sample rate\n",
    "            duration: Max duration in seconds\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sr = sr\n",
    "        self.duration = duration\n",
    "        self.max_samples = int(sr * duration)\n",
    "\n",
    "        # Feature extractor\n",
    "        self.feature_extractor = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sr,\n",
    "            n_fft=400,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            window_fn=torch.hamming_window,\n",
    "            n_mels=80,\n",
    "            f_min=20.0,\n",
    "            f_max=7600.0,\n",
    "            power=2.0\n",
    "        )\n",
    "        self.to_db = torchaudio.transforms.AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "        self.pairs = []\n",
    "        self.labels = []\n",
    "\n",
    "        with open(val_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 3:\n",
    "                    utt1, utt2, label = parts\n",
    "                    path1 = os.path.join(self.root_dir, utt1)\n",
    "                    path2 = os.path.join(self.root_dir, utt2)\n",
    "                    if os.path.exists(path1) and os.path.exists(path2):\n",
    "                        self.pairs.append((path1, path2))\n",
    "                        self.labels.append(int(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path1, path2 = self.pairs[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        feat1 = self._load_and_process(path1)  # [1, 80, T]\n",
    "        feat2 = self._load_and_process(path2)  # [1, 80, T]\n",
    "\n",
    "        return {\n",
    "            'input_values': feat1,       # [1, 80, T]\n",
    "            'input_values2': feat2,      # [1, 80, T]\n",
    "            'pair_labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _load_and_process(self, path):\n",
    "        waveform, sr = torchaudio.load(path)              # [1, T]\n",
    "        waveform = remove_silence(waveform, sample_rate=sr)  # [1, T]\n",
    "\n",
    "        # Pad or truncate\n",
    "        if waveform.shape[1] > self.max_samples:\n",
    "            waveform = waveform[:, :self.max_samples]\n",
    "        else:\n",
    "            pad = self.max_samples - waveform.shape[1]\n",
    "            waveform = F.pad(waveform, (0, pad))\n",
    "\n",
    "        mel = self.feature_extractor(waveform)\n",
    "        mel_db = self.to_db(mel)\n",
    "        mel_db = mel_db - mel_db.mean(dim=-1, keepdim=True)  # normalize\n",
    "        return mel_db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c16ced3",
   "metadata": {
    "papermill": {
     "duration": 0.006662,
     "end_time": "2025-07-19T02:43:43.428912",
     "exception": false,
     "start_time": "2025-07-19T02:43:43.422250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Validation Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8ed1602",
   "metadata": {
    "papermill": {
     "duration": 0.016269,
     "end_time": "2025-07-19T02:43:43.451835",
     "exception": false,
     "start_time": "2025-07-19T02:43:43.435566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValidationDataCollator:\n",
    "    def __call__(self, batch):\n",
    "        input_values = [\n",
    "            item['input_values'].squeeze()  # loại bỏ chiều dư nếu có\n",
    "            .transpose(0, 1)                # [80, T] → [T, 80]\n",
    "            for item in batch\n",
    "        ]\n",
    "        input_values2 = [\n",
    "            item['input_values2'].squeeze()  # loại bỏ chiều dư nếu có\n",
    "            .transpose(0, 1)                # [80, T] → [T, 80]\n",
    "            for item in batch\n",
    "        ]\n",
    "        pair_labels = torch.stack([item['pair_labels'] for item in batch])  # assume fixed-size\n",
    "\n",
    "        # Pad both input sets\n",
    "        input_values_padded = pad_sequence(input_values, batch_first=True)   # [B, T_max, 80]\n",
    "        input_values2_padded = pad_sequence(input_values2, batch_first=True) # [B, T_max, 80]\n",
    "\n",
    "        # Transpose back to [B, 1, 80, T_max]\n",
    "        input_values_padded = input_values_padded.transpose(1, 2).unsqueeze(1)\n",
    "        input_values2_padded = input_values2_padded.transpose(1, 2).unsqueeze(1)\n",
    "\n",
    "        return {\n",
    "            'input_values': input_values_padded,\n",
    "            'input_values2': input_values2_padded,\n",
    "            'pair_labels': pair_labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9ed37b1",
   "metadata": {
    "papermill": {
     "duration": 35.686724,
     "end_time": "2025-07-19T02:44:19.145871",
     "exception": false,
     "start_time": "2025-07-19T02:43:43.459147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random \n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "val_path = \"val_pairs.txt\"\n",
    "root_dir = \"data\"\n",
    "val_dataset = VSASV_Validation(val_path, root_dir)\n",
    "val_collator = ValidationDataCollator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd54dfd9",
   "metadata": {
    "papermill": {
     "duration": 0.015301,
     "end_time": "2025-07-19T02:44:19.168385",
     "exception": false,
     "start_time": "2025-07-19T02:44:19.153084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CombinedDataCollator:\n",
    "    def __init__(self, train_collator, val_collator):\n",
    "        self.train_collator = train_collator\n",
    "        self.val_collator = val_collator\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # Check if this is validation data by looking for input_values2\n",
    "        if isinstance(batch[0], dict) and 'input_values2' in batch[0]:\n",
    "            return self.val_collator(batch)\n",
    "        else:\n",
    "            return self.train_collator(batch)\n",
    "combined_collator = CombinedDataCollator(train_collator, val_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d88e55c",
   "metadata": {
    "papermill": {
     "duration": 0.006582,
     "end_time": "2025-07-19T02:44:19.181773",
     "exception": false,
     "start_time": "2025-07-19T02:44:19.175191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Set Up Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958be8f9",
   "metadata": {
    "papermill": {
     "duration": 0.006858,
     "end_time": "2025-07-19T02:44:19.195748",
     "exception": false,
     "start_time": "2025-07-19T02:44:19.188890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compute EER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfb92824",
   "metadata": {
    "papermill": {
     "duration": 0.020601,
     "end_time": "2025-07-19T02:44:19.223055",
     "exception": false,
     "start_time": "2025-07-19T02:44:19.202454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_eer(fnr, fpr, scores=None):\n",
    "    \"\"\"\n",
    "    Compute Equal Error Rate (EER) from false negative and false positive rates.\n",
    "    Returns: (eer, threshold)\n",
    "    \"\"\"\n",
    "    # Make sure fnr and fpr are numpy arrays\n",
    "    fnr = np.array(fnr)\n",
    "    fpr = np.array(fpr)\n",
    "    \n",
    "    # In case arrays are empty\n",
    "    if len(fnr) == 0 or len(fpr) == 0:\n",
    "        print(\"WARNING: Empty FNR or FPR arrays\")\n",
    "        return 0.5, 0.0  # Return default values\n",
    "    \n",
    "    # Calculate difference between FNR and FPR\n",
    "    diff = fnr - fpr\n",
    "    \n",
    "    # Find where the difference changes sign\n",
    "    # If diff changes sign, find the crossing point\n",
    "    if np.any(diff >= 0) and np.any(diff <= 0):\n",
    "        # Find indices where diff changes sign\n",
    "        positive_indices = np.flatnonzero(diff >= 0)\n",
    "        negative_indices = np.flatnonzero(diff <= 0)\n",
    "        \n",
    "        if len(positive_indices) > 0 and len(negative_indices) > 0:\n",
    "            # Get the boundary indices\n",
    "            idx1 = positive_indices[0]\n",
    "            idx2 = negative_indices[-1]\n",
    "            \n",
    "            # Check if indices are out of bounds\n",
    "            if idx1 >= len(fnr) or idx2 >= len(fnr):\n",
    "                print(\"WARNING: Index out of bounds\")\n",
    "                # Find closest points\n",
    "                abs_diff = np.abs(fnr - fpr)\n",
    "                min_idx = np.argmin(abs_diff)\n",
    "                eer = (fnr[min_idx] + fpr[min_idx]) / 2\n",
    "                threshold = scores[min_idx] if scores is not None else min_idx\n",
    "                return eer, threshold\n",
    "            \n",
    "            # Linear interpolation to find the EER\n",
    "            if fnr[idx1] == fpr[idx1]:\n",
    "                # Exactly equal at this point\n",
    "                eer = fnr[idx1]\n",
    "                threshold = scores[idx1] if scores is not None else idx1\n",
    "            else:\n",
    "                # Interpolate between idx1 and idx2\n",
    "                x = [fpr[idx2], fpr[idx1]]\n",
    "                y = [fnr[idx2], fnr[idx1]]\n",
    "                eer = np.mean(y)  # Approximate EER\n",
    "                \n",
    "                # If scores are provided, interpolate threshold\n",
    "                if scores is not None and idx1 < len(scores) and idx2 < len(scores):\n",
    "                    threshold = (scores[idx1] + scores[idx2]) / 2\n",
    "                else:\n",
    "                    threshold = (idx1 + idx2) / 2\n",
    "        else:\n",
    "            # Fallback if indices are not found\n",
    "            abs_diff = np.abs(fnr - fpr)\n",
    "            min_idx = np.argmin(abs_diff)\n",
    "            eer = (fnr[min_idx] + fpr[min_idx]) / 2\n",
    "            threshold = scores[min_idx] if scores is not None else min_idx\n",
    "    else:\n",
    "        # Fallback if no sign change - find the closest points\n",
    "        abs_diff = np.abs(fnr - fpr)\n",
    "        min_idx = np.argmin(abs_diff)\n",
    "        eer = (fnr[min_idx] + fpr[min_idx]) / 2\n",
    "        threshold = scores[min_idx] if scores is not None else min_idx\n",
    "    \n",
    "    return eer, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c348e96",
   "metadata": {
    "papermill": {
     "duration": 0.01914,
     "end_time": "2025-07-19T02:44:19.249554",
     "exception": false,
     "start_time": "2025-07-19T02:44:19.230414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_speaker_metrics(eval_pred):\n",
    "    \"\"\"Compute EER metrics for speaker verification.\"\"\"\n",
    "    # Extract embeddings and labels\n",
    "    embeddings1 = eval_pred.embeddings1\n",
    "    embeddings2 = eval_pred.embeddings2\n",
    "    pair_labels = eval_pred.labels\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarity_scores = np.array([\n",
    "        np.dot(e1, e2) / (np.linalg.norm(e1) * np.linalg.norm(e2) + 1e-10)\n",
    "        for e1, e2 in zip(embeddings1, embeddings2)\n",
    "    ])\n",
    "    \n",
    "    # Compute FPR and FNR\n",
    "    thresholds = np.sort(similarity_scores)\n",
    "    fpr = np.zeros(len(thresholds))\n",
    "    fnr = np.zeros(len(thresholds))\n",
    "    \n",
    "    for i, threshold in enumerate(thresholds):\n",
    "        # Predictions based on threshold\n",
    "        pred = (similarity_scores >= threshold).astype(int)\n",
    "        \n",
    "        # True positives, false positives, true negatives, false negatives\n",
    "        tp = np.sum((pred == 1) & (pair_labels == 1))\n",
    "        fp = np.sum((pred == 1) & (pair_labels == 0))\n",
    "        tn = np.sum((pred == 0) & (pair_labels == 0))\n",
    "        fn = np.sum((pred == 0) & (pair_labels == 1))\n",
    "        \n",
    "        # FPR and FNR\n",
    "        fpr[i] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr[i] = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    \n",
    "    # Calculate EER\n",
    "    eer, threshold = compute_eer(fnr, fpr, similarity_scores)\n",
    "    result = {\n",
    "        \"eer\": eer,\n",
    "        \"eer_threshold\": threshold\n",
    "    }\n",
    "    # Log EER to wandb directly\n",
    "    wandb.log({\"eer\": eer})\n",
    "    \n",
    "    # Create and log DET curve to wandb\n",
    "    if len(fpr) > 10:  # Only log if we have enough points\n",
    "        \n",
    "        # Log histogram of similarity scores\n",
    "        try:\n",
    "            wandb.log({\n",
    "                \"similarity_scores\": wandb.Histogram(similarity_scores),\n",
    "                \"same_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 1]),\n",
    "                \"diff_speaker_scores\": wandb.Histogram(similarity_scores[pair_labels == 0])\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error logging histograms: {e}\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69f874b7-cc2a-4044-afde-fb670a6b3c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class RandomSpeakerBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, speakers_per_batch=128, seed=42, infinite=True):\n",
    "        \"\"\"\n",
    "        dataset: Dataset phải có dataset.speakers (list speaker_id)\n",
    "                                 dataset.speaker_to_indices (dict: spk_id -> list index)\n",
    "        speakers_per_batch: số speaker mỗi batch (mỗi speaker 1 file)\n",
    "        seed: random seed\n",
    "        infinite: nếu True thì sampler chạy vô hạn (thường dùng với max_steps)\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.speakers = dataset.speakers\n",
    "        self.speaker_to_indices = dataset.speaker_to_indices\n",
    "        self.speakers_per_batch = speakers_per_batch\n",
    "        self.infinite = infinite\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "        assert len(self.speakers) >= speakers_per_batch, \\\n",
    "            f\"Số speaker ({len(self.speakers)}) nhỏ hơn {speakers_per_batch}\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            # chọn ngẫu nhiên 256 speaker\n",
    "            batch_speakers = self.rng.sample(self.speakers, self.speakers_per_batch)\n",
    "            batch_indices = []\n",
    "            for spk in batch_speakers:\n",
    "                # chọn ngẫu nhiên 1 utterance cho mỗi speaker\n",
    "                idx = self.rng.choice(self.speaker_to_indices[spk])\n",
    "                batch_indices.append(idx)\n",
    "            yield batch_indices\n",
    "\n",
    "            if not self.infinite:\n",
    "                break\n",
    "\n",
    "    def __len__(self):\n",
    "        # nếu muốn tính epoch dựa trên số speaker duyệt hết một vòng\n",
    "        return len(self.speakers) // self.speakers_per_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fddd38",
   "metadata": {
    "papermill": {
     "duration": 0.00656,
     "end_time": "2025-07-19T02:44:19.263224",
     "exception": false,
     "start_time": "2025-07-19T02:44:19.256664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## WANDB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d3379f4",
   "metadata": {
    "papermill": {
     "duration": 4.027948,
     "end_time": "2025-07-19T02:44:23.297869",
     "exception": false,
     "start_time": "2025-07-19T02:44:19.269921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (0.5.3)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.14.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.8)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from wandb) (25.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.45)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.34.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.2.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "211d5ebc",
   "metadata": {
    "papermill": {
     "duration": 0.160576,
     "end_time": "2025-07-19T02:44:23.467011",
     "exception": false,
     "start_time": "2025-07-19T02:44:23.306435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "# Log model architecture to wandb\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ResNet48_ASV().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1706db6c",
   "metadata": {
    "papermill": {
     "duration": 3.609111,
     "end_time": "2025-07-19T02:44:27.083323",
     "exception": false,
     "start_time": "2025-07-19T02:44:23.474212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import torch\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3475e16c",
   "metadata": {
    "papermill": {
     "duration": 0.016004,
     "end_time": "2025-07-19T02:44:27.107203",
     "exception": false,
     "start_time": "2025-07-19T02:44:27.091199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"ResNet48_ASV_WithAAM\",\n",
    "    \"embedding_dim\": 256,\n",
    "    \"num_classes\": 652,\n",
    "    \"features\": \"MFCC\",\n",
    "    \"loss\": \"AAMSoftmax\",\n",
    "    \n",
    "    # Optimizer & Learning\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \n",
    "    # Batch & Epochs\n",
    "    \"batch_size_per_device\": 128,\n",
    "    \"global_batch_size\": 128,\n",
    "    \"num_train_epochs\": 50,\n",
    "    \n",
    "    # Evaluation & Saving\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"save_strategy\": \"steps\",\n",
    "    \"save_total_limit\": 45,\n",
    "    \"metric_for_best_model\": \"eval_eer\",\n",
    "    \"greater_is_better\": False,  # Lower EER is better\n",
    "    \n",
    "    # Logging\n",
    "    \"logging_steps\": 500,\n",
    "    \"report_to\": \"wandb\",\n",
    "    \n",
    "    # Hardware\n",
    "    \"fp16\": True,\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \n",
    "    # Misc\n",
    "    \"load_best_model_at_end\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ea6cc3d",
   "metadata": {
    "papermill": {
     "duration": 2.392503,
     "end_time": "2025-07-19T02:44:29.575468",
     "exception": false,
     "start_time": "2025-07-19T02:44:27.182965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhddat2k4\u001b[0m (\u001b[33mhddat2k4-uit\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250805_061537-oqwj3fni</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hddat2k4-uit/ResNet48-ASV-final/runs/oqwj3fni' target=\"_blank\">ResNet48-AAM-MFBE-60ksteps</a></strong> to <a href='https://wandb.ai/hddat2k4-uit/ResNet48-ASV-final' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hddat2k4-uit/ResNet48-ASV-final' target=\"_blank\">https://wandb.ai/hddat2k4-uit/ResNet48-ASV-final</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hddat2k4-uit/ResNet48-ASV-final/runs/oqwj3fni' target=\"_blank\">https://wandb.ai/hddat2k4-uit/ResNet48-ASV-final/runs/oqwj3fni</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hddat2k4-uit/ResNet48-ASV-final/runs/oqwj3fni?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f6cec15c8e0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_KEY\"] = \"4b8af864ea6d5ec9af172b42a4c40e4444e20cf7\"\n",
    "wandb.login(key=os.getenv(\"WANDB_KEY\"))\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"ResNet48-ASV-final\",\n",
    "    name=\"ResNet48-AAM-MFBE-60ksteps\",\n",
    "    config=config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07442c79",
   "metadata": {
    "papermill": {
     "duration": 0.007207,
     "end_time": "2025-07-19T02:44:29.590617",
     "exception": false,
     "start_time": "2025-07-19T02:44:29.583410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93f36f43",
   "metadata": {
    "papermill": {
     "duration": 37.205005,
     "end_time": "2025-07-19T02:45:06.803595",
     "exception": false,
     "start_time": "2025-07-19T02:44:29.598590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "class VSASV_Trainer(Trainer):\n",
    "    def __init__(self, *args, total_steps=50000, margin=0.2, scale=30, num_speakers=815, embedding_dim=256, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.total_steps = total_steps\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        self.num_speakers = num_speakers\n",
    "        self.use_amp = self.args.fp16\n",
    "        ##Initialize AAMSoftmax Loss\n",
    "        self.criterion = AAMSoftmax(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_classes=num_speakers,\n",
    "            m=margin,\n",
    "            s=scale\n",
    "        ).to(self.args.device)\n",
    "\n",
    "        self.pairs_embeddings1 = []\n",
    "        self.pairs_embeddings2 = []\n",
    "        self.pairs_labels = []\n",
    "\n",
    "        wandb.config.update({\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"aam_margin\": margin,\n",
    "            \"aam_scale\": scale,\n",
    "            \"num_speakers\": num_speakers\n",
    "        })\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is None:\n",
    "            decay_params = []\n",
    "            head_params = []\n",
    "\n",
    "            for name, param in self.model.named_parameters():\n",
    "                if not param.requires_grad:\n",
    "                    continue\n",
    "                if \"classifier\" in name or \"am_softmax\" in name:\n",
    "                    head_params.append(param)   # AM-Softmax head\n",
    "                else:\n",
    "                    decay_params.append(param)  # Backbone\n",
    "\n",
    "            optimizer_grouped_parameters = [\n",
    "                {\"params\": decay_params, \"weight_decay\": 1e-5},\n",
    "                {\"params\": head_params, \"weight_decay\": 1e-4},\n",
    "            ]\n",
    "\n",
    "            self.optimizer = torch.optim.AdamW(\n",
    "                optimizer_grouped_parameters,\n",
    "                lr=self.args.learning_rate,\n",
    "                betas=(0.9, 0.98),\n",
    "                eps=1e-8\n",
    "            )\n",
    "        return self.optimizer\n",
    "\n",
    "    def create_scheduler(self, num_training_steps, optimizer=None):\n",
    "        if self.lr_scheduler is None:\n",
    "            def lr_lambda(step):\n",
    "                if step < 10000:  # giữ nguyên trong 10k step đầu\n",
    "                    return 1.0\n",
    "                return 0.5 ** ((step - 10000) // 4000)\n",
    "            \n",
    "            self.lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "                self.optimizer, lr_lambda=lr_lambda\n",
    "            )\n",
    "        return self.lr_scheduler\n",
    "\n",
    "\n",
    "    def get_margin(self, epoch):\n",
    "        \"\"\"Schedule margin theo mô tả paper\"\"\"\n",
    "        if epoch <= 10:\n",
    "            return 0.0\n",
    "        elif 4 <= epoch <= 13:\n",
    "            # tăng tuyến tính từ 0 → 0.3 trong 10 epoch\n",
    "            progress = (epoch - 3) / 10.0\n",
    "            return 0.3 * progress\n",
    "        else:\n",
    "            return 0.3\n",
    "    def get_train_dataloader(self):\n",
    "        from torch.utils.data import DataLoader\n",
    "    \n",
    "        # tạo batch_sampler\n",
    "        batch_sampler = RandomSpeakerBatchSampler(\n",
    "            self.train_dataset, \n",
    "            speakers_per_batch=self.args.train_batch_size,  # dùng train_batch_size làm số speaker/batch\n",
    "            infinite=True\n",
    "        )\n",
    "    \n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_sampler=batch_sampler,   # thay batch_size + shuffle\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=4,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        \"\"\"Create a working dataloader for evaluation\"\"\"\n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        return DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=self.args.eval_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.data_collator,\n",
    "            num_workers=4,  # Critical: use single process\n",
    "            pin_memory=True,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "\n",
    "    def training_step(self, model, inputs, num_items=None):\n",
    "        # Lấy epoch hiện tại\n",
    "        current_epoch = int(self.state.global_step // 1000) + 1  # vì 1 epoch = 5000 steps\n",
    "        new_margin = self.get_margin(current_epoch)\n",
    "\n",
    "        # update margin trong loss\n",
    "        if hasattr(self.criterion, \"margin\"):\n",
    "            self.criterion.margin = new_margin\n",
    "\n",
    "        # logging\n",
    "        if self.state.global_step % self.args.logging_steps == 0:\n",
    "            self.log({\"margin\": new_margin})\n",
    "            wandb.log({\"margin\": new_margin})\n",
    "            print(f\"Epoch {current_epoch}: margin = {new_margin:.4f}\")\n",
    "\n",
    "        return super().training_step(model, inputs, num_items)\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"Compute AAMSoftmax loss for the speaker embeddings.\"\"\"\n",
    "        # Extract inputs\n",
    "        input_values = inputs.get('input_values')\n",
    "        labels = inputs.get('speaker_labels')\n",
    "    \n",
    "        device = next(model.parameters()).device\n",
    "        input_values = input_values.to(device)\n",
    "        labels = labels.to(device) if labels is not None else None\n",
    "        \n",
    "        # Handle evaluation inputs with pairs for EER computation\n",
    "        is_eval_with_pairs = False\n",
    "        if not model.training and inputs.get('input_values2') is not None:\n",
    "            is_eval_with_pairs = True\n",
    "            input_values2 = inputs.get('input_values2').to(device)\n",
    "            pair_labels = inputs.get('pair_labels').to(device)\n",
    "        \n",
    "        # Forward pass to get speaker embeddings\n",
    "        embeddings = model(input_values)\n",
    "        \n",
    "        # Handle evaluation with pairs for EER\n",
    "        if is_eval_with_pairs:\n",
    "            # Get embeddings for second utterance in pairs\n",
    "            embeddings2 = model(input_values2)\n",
    "            \n",
    "            # Store pairs for EER calculation\n",
    "            self.pairs_embeddings1.append(embeddings.detach().cpu())\n",
    "            self.pairs_embeddings2.append(embeddings2.detach().cpu())\n",
    "            self.pairs_labels.append(pair_labels.detach().cpu())\n",
    "        \n",
    "        # Use AAMSoftmax loss for training\n",
    "        if labels is not None:\n",
    "            loss, outputs = self.criterion(embeddings, labels)\n",
    "            \n",
    "            # Log loss to wandb during training\n",
    "            if self.model.training and self.state.global_step % self.args.logging_steps == 0:\n",
    "                wandb.log({\"train/aam_loss\": loss.item()})\n",
    "        else:\n",
    "            loss = None\n",
    "            outputs = None\n",
    "        torch.cuda.empty_cache()\n",
    "        if return_outputs:\n",
    "            return loss, {\"loss\": loss, \"logits\": outputs, \"embeddings\": embeddings}\n",
    "        else:\n",
    "            return loss\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        with torch.no_grad():\n",
    "            device = next(model.parameters()).device\n",
    "    \n",
    "            # Forward input1\n",
    "            embeddings1 = model(inputs[\"input_values\"].to(device))\n",
    "            \n",
    "            # Forward input2 (nếu có)\n",
    "            if \"input_values2\" in inputs and \"pair_labels\" in inputs:\n",
    "                embeddings2 = model(inputs[\"input_values2\"].to(device))\n",
    "    \n",
    "                # Lưu embedding & nhãn để tính EER\n",
    "                self.pairs_embeddings1.append(embeddings1.detach().cpu())\n",
    "                self.pairs_embeddings2.append(embeddings2.detach().cpu())\n",
    "                self.pairs_labels.append(inputs[\"pair_labels\"].detach().cpu())\n",
    "    \n",
    "            # Nếu có label chính (ví dụ speaker_labels) thì tính loss cho HuggingFace Trainer\n",
    "            if \"speaker_labels\" in inputs:\n",
    "                outputs = model(inputs[\"input_values\"].to(device))\n",
    "                loss = self.criterion(outputs, inputs[\"speaker_labels\"].to(device))\n",
    "            else:\n",
    "                loss = None\n",
    "    \n",
    "        return (loss, None, None)\n",
    "\n",
    "      \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"Override evaluate to compute EER at the end of evaluation.\"\"\"\n",
    "        # Reset storage for pairs\n",
    "        self.pairs_embeddings1 = []\n",
    "        self.pairs_embeddings2 = []\n",
    "        self.pairs_labels = []\n",
    "        \n",
    "        # Run standard evaluation\n",
    "        metrics = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # Calculate EER if we have collected pairs\n",
    "        if len(self.pairs_embeddings1) > 0:\n",
    "            # Prepare data for compute metrics function\n",
    "            embeddings1 = torch.cat(self.pairs_embeddings1, dim=0).numpy()\n",
    "            embeddings2 = torch.cat(self.pairs_embeddings2, dim=0).numpy()\n",
    "            pair_labels = torch.cat(self.pairs_labels, dim=0).numpy()\n",
    "            \n",
    "            # Create a container class to hold the embeddings\n",
    "            class EmbeddingPairs:\n",
    "                def __init__(self, embeddings1, embeddings2, labels):\n",
    "                    self.embeddings1 = embeddings1\n",
    "                    self.embeddings2 = embeddings2\n",
    "                    self.labels = labels\n",
    "            \n",
    "            eval_pairs = EmbeddingPairs(embeddings1, embeddings2, pair_labels)\n",
    "            \n",
    "            # Compute EER metrics\n",
    "            eer_metrics = compute_speaker_metrics(eval_pairs)\n",
    "            \n",
    "            # Add EER metrics to the overall metrics\n",
    "            for key, value in eer_metrics.items():\n",
    "                if key not in metrics:\n",
    "                    metrics[f\"{metric_key_prefix}_{key}\"] = value\n",
    "            \n",
    "            # Log to wandb with correct prefix\n",
    "            wandb_metrics = {\n",
    "                f\"{metric_key_prefix}/{key}\": value \n",
    "                for key, value in metrics.items() \n",
    "                if key.startswith(metric_key_prefix)\n",
    "            }\n",
    "            wandb.log(wandb_metrics)\n",
    "            \n",
    "            print(f\"\\n{metric_key_prefix.capitalize()} EER: {metrics.get(f'{metric_key_prefix}_eer', 0):.4f}\")\n",
    "            \n",
    "            # Log embedding visualizations to wandb (t-SNE of random subset)\n",
    "            if len(embeddings1) > 100:\n",
    "                try:\n",
    "                    from sklearn.manifold import TSNE\n",
    "                    # Sample a subset for visualization (for efficiency)\n",
    "                    max_samples = min(500, len(embeddings1))\n",
    "                    indices = np.random.choice(len(embeddings1), max_samples, replace=False)\n",
    "                    \n",
    "                    # Apply t-SNE\n",
    "                    tsne = TSNE(n_components=2, random_state=42)\n",
    "                    embeddings_combined = np.vstack([embeddings1[indices], embeddings2[indices]])\n",
    "                    embeddings_2d = tsne.fit_transform(embeddings_combined)\n",
    "                    \n",
    "                    # Split back into two sets\n",
    "                    n_samples = len(indices)\n",
    "                    embeddings1_2d = embeddings_2d[:n_samples]\n",
    "                    embeddings2_2d = embeddings_2d[n_samples:]\n",
    "                    \n",
    "                    # Create scatter plot data\n",
    "                    data = []\n",
    "                    for i in range(n_samples):\n",
    "                        data.append([\n",
    "                            embeddings1_2d[i, 0], embeddings1_2d[i, 1], \n",
    "                            \"Embedding 1\", int(pair_labels[indices[i]])\n",
    "                        ])\n",
    "                        data.append([\n",
    "                            embeddings2_2d[i, 0], embeddings2_2d[i, 1], \n",
    "                            \"Embedding 2\", int(pair_labels[indices[i]])\n",
    "                        ])\n",
    "                    \n",
    "                    # Log to wandb\n",
    "                    wandb.log({\n",
    "                        f\"{metric_key_prefix}/embeddings_tsne\": wandb.Table(\n",
    "                            data=data,\n",
    "                            columns=[\"x\", \"y\", \"embedding_type\", \"same_speaker\"]\n",
    "                        )\n",
    "                    })\n",
    "                except ImportError:\n",
    "                    print(\"sklearn not available for t-SNE visualization\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error creating t-SNE visualization: {e}\")\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55865134-2afd-4d1c-8656-e5a8d183c8f5",
   "metadata": {
    "papermill": {
     "duration": 0.007485,
     "end_time": "2025-07-19T02:45:06.819204",
     "exception": false,
     "start_time": "2025-07-19T02:45:06.811719",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c62c3760",
   "metadata": {
    "papermill": {
     "duration": 0.021672,
     "end_time": "2025-07-19T02:45:06.848522",
     "exception": false,
     "start_time": "2025-07-19T02:45:06.826850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoint_ResNet481\",\n",
    "    per_device_train_batch_size=128,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=500,\n",
    "    per_device_eval_batch_size=128,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_total_limit=101,\n",
    "    max_steps=50000,\n",
    "    #warmup_steps=4000,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=[\"wandb\"],  # Enable logging to wandb\n",
    "    metric_for_best_model=\"eval_eer\",\n",
    "    greater_is_better=False,  # Lower EER is better\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "003bb1e6-5485-4be3-87f3-2e9ec850c3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292005"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c76fb64",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-07-19T02:45:06.856546",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: margin = 0.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8001' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 8001/50000 3:05:31 < 16:14:05, 0.72 it/s, Epoch 1600/10000]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.612300</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.291900</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.593400</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.375600</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>No log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='191' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [191/390 03:47 < 03:58, 0.83 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: margin = 0.0000\n",
      "\n",
      "Eval EER: 0.2732\n",
      "Epoch 2: margin = 0.0000\n",
      "Epoch 2: margin = 0.0000\n",
      "\n",
      "Eval EER: 0.2878\n",
      "Epoch 3: margin = 0.0000\n",
      "Epoch 3: margin = 0.0000\n",
      "\n",
      "Eval EER: 0.2777\n",
      "Epoch 4: margin = 0.0000\n",
      "Epoch 4: margin = 0.0000\n",
      "\n",
      "Eval EER: 0.2686\n",
      "Epoch 5: margin = 0.0000\n",
      "Epoch 5: margin = 0.0000\n",
      "\n",
      "Eval EER: 0.2608\n",
      "Epoch 6: margin = 0.0000\n",
      "Epoch 6: margin = 0.0000\n",
      "\n",
      "Eval EER: 0.2614\n",
      "Epoch 7: margin = 0.0000\n",
      "Epoch 7: margin = 0.0000\n",
      "\n",
      "Eval EER: 0.2581\n",
      "Epoch 8: margin = 0.0000\n",
      "Epoch 8: margin = 0.0000\n"
     ]
    }
   ],
   "source": [
    "trainer = VSASV_Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset, \n",
    "    eval_dataset=val_dataset,    \n",
    "    data_collator=combined_collator,  \n",
    "    compute_metrics=compute_speaker_metrics,\n",
    "    total_steps=int(training_args.max_steps),\n",
    "    num_speakers=652,  # Adjust based on your dataset\n",
    "    margin=0,\n",
    "    scale=30,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "# Tiếp tục từ checkpoint step 15000\n",
    "#trainer.train(resume_from_checkpoint=\"./checkpoint_ResNet48/checkpoint-15000\")\n",
    "\n",
    "# Save best model (automatically saved if using metric_for_best_model + load_best_model_at_end=True)\n",
    "trainer.save_model(os.path.join(training_args.output_dir, \"best_model\"))\n",
    "\n",
    "# Save training state (optimizer, scheduler, etc.)\n",
    "trainer.save_state()\n",
    "\n",
    "# Log to wandb\n",
    "wandb.save(os.path.join(training_args.output_dir, \"best_model\", \"*\"))\n",
    "wandb.save(os.path.join(training_args.output_dir, \"trainer_state.json\"))\n",
    "wandb.save(os.path.join(training_args.output_dir, \"optimizer.pt\"))\n",
    "wandb.save(os.path.join(training_args.output_dir, \"scheduler.pt\"))\n",
    "\n",
    "# Finish wandb run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7842279,
     "sourceId": 12432698,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7850760,
     "sourceId": 12445751,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7852358,
     "sourceId": 12448151,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7896230,
     "sourceId": 12510265,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7896850,
     "sourceId": 12511172,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7897637,
     "sourceId": 12512443,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-19T02:43:20.774057",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
