{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76730c8-4921-4155-b64c-1b6fdc8a046b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m161.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.7.31-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m568.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.6.15)\n",
      "Downloading transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.7.31-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (804 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m804.5/804.5 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.6/199.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.7.0 hf-xet-1.1.5 huggingface-hub-0.34.3 regex-2025.7.31 safetensors-0.5.3 tokenizers-0.21.4 transformers-4.54.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5bfae0-8eff-42af-9295-7490fb3abe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số thư mục trong data: 816\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Đường dẫn thư mục data\n",
    "path = \"data\"\n",
    "\n",
    "# Đếm số folder con\n",
    "num_dirs = sum(1 for item in os.listdir(path) if os.path.isdir(os.path.join(path, item)))\n",
    "print(\"Số thư mục trong data:\", num_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e22d51-75ab-4b25-bce8-97076f1db8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tạo file full_path.txt thành công ✅\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_dir = \"data\"\n",
    "output_file = \"full_path.txt\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Duyệt qua từng speaker folder (idxxxxx)\n",
    "    for speaker in sorted(os.listdir(base_dir)):\n",
    "        speaker_path = os.path.join(base_dir, speaker)\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "        \n",
    "        # Duyệt qua nhãn (spoof, bonafide)\n",
    "        for label in [\"spoof\", \"bonafide\"]:\n",
    "            label_path = os.path.join(speaker_path, label)\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue\n",
    "            \n",
    "            # Duyệt qua từng file audio\n",
    "            for wav in sorted(os.listdir(label_path)):\n",
    "                if wav.endswith(\".wav\"):\n",
    "                    rel_path = os.path.join(speaker, label, wav)  # relative path\n",
    "                    line = f\"{speaker} {rel_path} {label}\\n\"\n",
    "                    f.write(line)\n",
    "\n",
    "print(f\"Tạo file {output_file} thành công ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762fec37-0b12-4bfc-a186-7caa87701e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "def make_train_val_pairs(all_file, train_file=\"train.txt\", val_file=\"val.txt\", val_pairs_file=\"val_pairs.txt\",\n",
    "                         split_ratio=0.8, max_pairs=50000, seed=42):\n",
    "    random.seed(seed)\n",
    "\n",
    "    # B1: Gom file theo speaker, chỉ lấy bonafide (label == \"0\")\n",
    "    speaker2lines = defaultdict(list)\n",
    "    with open(all_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            spk, path, label = line.strip().split()\n",
    "            if label == \"bonafide\":  # chỉ giữ bonafide\n",
    "                speaker2lines[spk].append((spk, path, label))\n",
    "\n",
    "    speakers = list(speaker2lines.keys())\n",
    "    random.shuffle(speakers)\n",
    "\n",
    "    # B2: Chia speaker train/val\n",
    "    n_train = int(len(speakers) * split_ratio)\n",
    "    train_speakers = set(speakers[:n_train])\n",
    "    val_speakers = set(speakers[n_train:])\n",
    "\n",
    "    # B3: Xuất train.txt và val.txt (val chỉ giữ file chứa \"orig\")\n",
    "    with open(train_file, \"w\") as f_train, open(val_file, \"w\") as f_val:\n",
    "        for spk in train_speakers:\n",
    "            for entry in speaker2lines[spk]:\n",
    "                f_train.write(\" \".join(entry) + \"\\n\")\n",
    "        for spk in val_speakers:\n",
    "            for entry in speaker2lines[spk]:\n",
    "                if \"orig\" in entry[1]:  # chỉ lấy file có 'orig' trong tên\n",
    "                    f_val.write(\" \".join(entry) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ Train: {len(train_speakers)} speakers → {train_file}\")\n",
    "    print(f\"✅ Val:   {len(val_speakers)} speakers (only bonafide 'orig' files) → {val_file}\")\n",
    "\n",
    "    # B4: Sinh val pairs (giới hạn 20k cặp, chỉ bonafide)\n",
    "    val_files_by_spk = {spk: [path for _, path, _ in lines if \"orig\" in path]\n",
    "                        for spk, lines in speaker2lines.items() if spk in val_speakers}\n",
    "    val_speakers_list = [spk for spk, files in val_files_by_spk.items() if len(files) > 0]\n",
    "\n",
    "    # Positive pairs\n",
    "    pos_pairs = []\n",
    "    for spk, files in val_files_by_spk.items():\n",
    "        if len(files) < 2:\n",
    "            continue\n",
    "        pos_pairs.extend([(f1, f2, 1) for f1, f2 in itertools.combinations(files, 2)])\n",
    "\n",
    "    random.shuffle(pos_pairs)\n",
    "    n_pos = min(len(pos_pairs), max_pairs // 2)  # 10k positive\n",
    "    val_pairs = pos_pairs[:n_pos]\n",
    "\n",
    "    # Negative pairs\n",
    "    n_neg = n_pos\n",
    "    while len(val_pairs) < n_pos + n_neg and len(val_speakers_list) >= 2:\n",
    "        spk1, spk2 = random.sample(val_speakers_list, 2)\n",
    "        f1 = random.choice(val_files_by_spk[spk1])\n",
    "        f2 = random.choice(val_files_by_spk[spk2])\n",
    "        val_pairs.append((f1, f2, 0))\n",
    "\n",
    "    random.shuffle(val_pairs)\n",
    "\n",
    "    # Xuất val_pairs.txt\n",
    "    with open(val_pairs_file, \"w\") as f:\n",
    "        for f1, f2, label in val_pairs:\n",
    "            f.write(f\"{f1} {f2} {label}\\n\")\n",
    "\n",
    "    print(f\"✅ Val pairs: {len(val_pairs)} (≈{n_pos} pos + {n_neg} neg, only bonafide) → {val_pairs_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e7f3e00-8c91-4332-969c-1fc4b528b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train: 652 speakers → train.txt\n",
      "✅ Val:   163 speakers (only bonafide 'orig' files) → val.txt\n",
      "✅ Val pairs: 50000 (≈25000 pos + 25000 neg, only bonafide) → val_pairs.txt\n"
     ]
    }
   ],
   "source": [
    "make_train_val_pairs(\"full_path.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5901d035-6a4c-4c3d-9464-3a8de1dfc4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số speaker đã có trong ASV train: 652\n"
     ]
    }
   ],
   "source": [
    "# file chứa các speaker đã train ASV\n",
    "asv_train_file = \"train.txt\"\n",
    "asv_ids = set()\n",
    "with open(asv_train_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        spk_id = line.strip().split()[0]   # cột 1\n",
    "        asv_ids.add(spk_id)\n",
    "\n",
    "print(\"Số speaker đã có trong ASV train:\", len(asv_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1425f710-8c94-4647-949e-68703014a9fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số speaker còn lại: 163\n"
     ]
    }
   ],
   "source": [
    "# file full\n",
    "full_file = \"full_path.txt\"\n",
    "spk2utts = {}\n",
    "\n",
    "with open(full_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        spk, path, label = line.strip().split()\n",
    "        if spk not in asv_ids:   # chỉ lấy speaker chưa có trong ASV train\n",
    "            spk2utts.setdefault(spk, []).append((path, label))\n",
    "\n",
    "print(\"Số speaker còn lại:\", len(spk2utts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "631b948e-8849-4c84-b9a4-8c8264357545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Speakers trong ASV train: 652\n",
      "✅ Speakers còn lại để tạo trial: 163\n",
      "✅ Saved trials: 84101 Target: 25000 Nontarget: 59101\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "# 1. Load speaker IDs đã train ASV\n",
    "asv_ids = set()\n",
    "with open(\"train.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        spk_id = line.strip().split()[0]  # cột 1 = speaker id\n",
    "        asv_ids.add(spk_id)\n",
    "\n",
    "print(f\"✅ Speakers trong ASV train: {len(asv_ids)}\")\n",
    "\n",
    "# 2. Load metadata gốc, chỉ giữ speaker chưa có trong ASV train\n",
    "spk2bonafide = defaultdict(list)\n",
    "spk2spoof = defaultdict(list)\n",
    "\n",
    "with open(\"full_path.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        spk, path, label = line.strip().split()\n",
    "        if spk in asv_ids:   # bỏ qua speaker đã train\n",
    "            continue\n",
    "        if label == \"bonafide\":\n",
    "            spk2bonafide[spk].append(path)\n",
    "        elif label == \"spoof\":\n",
    "            spk2spoof[spk].append(path)\n",
    "\n",
    "speakers = list(spk2bonafide.keys())\n",
    "print(f\"✅ Speakers còn lại để tạo trial: {len(speakers)}\")\n",
    "\n",
    "# 3. Sinh trials\n",
    "target_trials = []\n",
    "\n",
    "# Target: bonafide vs bonafide cùng speaker\n",
    "for spk, utts in spk2bonafide.items():\n",
    "    if len(utts) < 2:\n",
    "        continue\n",
    "    pairs = [(a, b) for i, a in enumerate(utts) for b in utts[i+1:]]\n",
    "    for a, b in pairs:\n",
    "        target_trials.append((a, b, \"target\"))\n",
    "\n",
    "# Nontarget = tất cả còn lại\n",
    "nontarget_a, nontarget_b, nontarget_c = [], [], []\n",
    "\n",
    "# a) bonafide speaker A vs bonafide speaker B\n",
    "for i in range(len(speakers)):\n",
    "    for j in range(i+1, len(speakers)):\n",
    "        u1 = random.choice(spk2bonafide[speakers[i]])\n",
    "        u2 = random.choice(spk2bonafide[speakers[j]])\n",
    "        nontarget_a.append((u1, u2, \"nontarget\"))\n",
    "\n",
    "# b) bonafide vs spoof (same speaker)\n",
    "for spk, bona in spk2bonafide.items():\n",
    "    for b in bona:\n",
    "        for s in spk2spoof.get(spk, []):\n",
    "            nontarget_b.append((b, s, \"nontarget\"))\n",
    "\n",
    "# c) bonafide vs spoof (different speakers)\n",
    "for spk1 in speakers:\n",
    "    for spk2 in spk2spoof:\n",
    "        if spk1 == spk2:\n",
    "            continue\n",
    "        b = random.choice(spk2bonafide[spk1])\n",
    "        s = random.choice(spk2spoof[spk2])\n",
    "        nontarget_c.append((b, s, \"nontarget\"))\n",
    "\n",
    "# ---- Balance: lấy 25k mỗi loại ----\n",
    "nontarget_a = random.sample(nontarget_a, min(25000, len(nontarget_a)))\n",
    "nontarget_b = random.sample(nontarget_b, min(25000, len(nontarget_b)))\n",
    "nontarget_c = random.sample(nontarget_c, min(25000, len(nontarget_c)))\n",
    "\n",
    "# Gộp lại\n",
    "nontarget_trials = nontarget_a + nontarget_b + nontarget_c\n",
    "\n",
    "# 4. Balance: lấy 25k target và 25k nontarget\n",
    "target_trials = random.sample(target_trials, min(25000, len(target_trials)))\n",
    "\n",
    "all_trials = target_trials + nontarget_trials\n",
    "random.shuffle(all_trials)\n",
    "\n",
    "# 5. Save\n",
    "with open(\"sasv_binary_trials.txt\", \"w\") as f:\n",
    "    for enroll, test, label in all_trials:\n",
    "        f.write(f\"{enroll} {test} {label}\\n\")\n",
    "\n",
    "print(\"✅ Saved trials:\", len(all_trials), \"Target:\", len(target_trials), \"Nontarget:\", len(nontarget_trials))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (System)",
   "language": "python",
   "name": "system-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
