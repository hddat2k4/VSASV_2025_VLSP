{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12430756,"sourceType":"datasetVersion","datasetId":7841008},{"sourceId":12511172,"sourceType":"datasetVersion","datasetId":7896850},{"sourceId":12592840,"sourceType":"datasetVersion","datasetId":7953688},{"sourceId":12596974,"sourceType":"datasetVersion","datasetId":7956364},{"sourceId":12655743,"sourceType":"datasetVersion","datasetId":7997816}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import requests, pathlib, os, zipfile, gdown, time\nimport torch, torchaudio, librosa, soundfile, math, numpy, random\nimport torch.nn.functional as F\n\nfrom pathlib import Path\nfrom torch import nn, Tensor\nfrom tqdm import tqdm ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.006238Z","iopub.execute_input":"2025-08-03T04:13:25.006477Z","iopub.status.idle":"2025-08-03T04:13:25.012231Z","shell.execute_reply.started":"2025-08-03T04:13:25.006460Z","shell.execute_reply":"2025-08-03T04:13:25.011432Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"# Create Train/Val","metadata":{}},{"cell_type":"code","source":"# import os\n# import random\n\n# def split_speakers_with_spoof(base_dir, train_list_file, val_list_file, train_ratio=0.8, seed=42):\n#     random.seed(seed)\n\n#     # Láº¥y toÃ n bá»™ speakers cÃ³ folder spoof\n#     speakers = []\n#     for d in os.listdir(base_dir):\n#         spk_path = os.path.join(base_dir, d)\n#         spoof_path = os.path.join(spk_path, \"spoof\")\n#         if os.path.isdir(spk_path) and os.path.exists(spoof_path) and len(os.listdir(spoof_path)) > 0:\n#             speakers.append(d)\n\n#     speakers.sort()\n\n#     # Shuffle vÃ  split\n#     random.shuffle(speakers)\n#     n_train = int(len(speakers) * train_ratio)\n#     train_speakers = speakers[:n_train]\n#     val_speakers = speakers[n_train:]\n\n#     print(f\"Total speakers with spoof: {len(speakers)} | Train: {len(train_speakers)} | Val: {len(val_speakers)}\")\n\n#     def write_list(speaker_subset, output_file):\n#         with open(output_file, \"w\") as f:\n#             for speaker in speaker_subset:\n#                 spk_path = os.path.join(base_dir, speaker)\n#                 for label in [\"bonafide\", \"spoof\"]:\n#                     label_path = os.path.join(spk_path, label)\n#                     if not os.path.exists(label_path):\n#                         continue\n#                     for wav in os.listdir(label_path):\n#                         if wav.endswith(\".wav\"):\n#                             filepath = os.path.join(label_path, wav)\n#                             f.write(f\"{filepath} {label}\\n\")\n\n#     # Ghi file train/val\n#     write_list(train_speakers, train_list_file)\n#     write_list(val_speakers, val_list_file)\n\n# # VÃ­ dá»¥ dÃ¹ng\n# split_speakers_with_spoof(\"/kaggle/input/vlsp-train/home4/vuhl/VSASV-Dataset/vlsp2025/train\", \"train_list.txt\", \"val_list.txt\", train_ratio=0.8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.014671Z","iopub.execute_input":"2025-08-03T04:13:25.014892Z","iopub.status.idle":"2025-08-03T04:13:25.030742Z","shell.execute_reply.started":"2025-08-03T04:13:25.014876Z","shell.execute_reply":"2025-08-03T04:13:25.029890Z"}},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":"# AASIST Model","metadata":{}},{"cell_type":"code","source":"from typing import Union\nimport numpy as np\n\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, **kwargs):\n        super().__init__()\n\n        # attention map\n        self.att_proj = nn.Linear(in_dim, out_dim)\n        self.att_weight = self._init_new_params(out_dim, 1)\n\n        # project\n        self.proj_with_att = nn.Linear(in_dim, out_dim)\n        self.proj_without_att = nn.Linear(in_dim, out_dim)\n\n        # batch norm\n        self.bn = nn.BatchNorm1d(out_dim)\n\n        # dropout for inputs\n        self.input_drop = nn.Dropout(p=0.2)\n\n        # activate\n        self.act = nn.SELU(inplace=True)\n\n        # temperature\n        self.temp = 1.\n        if \"temperature\" in kwargs:\n            self.temp = kwargs[\"temperature\"]\n\n    def forward(self, x):\n        '''\n        x   :(#bs, #node, #dim)\n        '''\n        # apply input dropout\n        x = self.input_drop(x)\n\n        # derive attention map\n        att_map = self._derive_att_map(x)\n\n        # projection\n        x = self._project(x, att_map)\n\n        # apply batch norm\n        x = self._apply_BN(x)\n        x = self.act(x)\n        return x\n\n    def _pairwise_mul_nodes(self, x):\n        '''\n        Calculates pairwise multiplication of nodes.\n        - for attention map\n        x           :(#bs, #node, #dim)\n        out_shape   :(#bs, #node, #node, #dim)\n        '''\n\n        nb_nodes = x.size(1)\n        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n        x_mirror = x.transpose(1, 2)\n\n        return x * x_mirror\n\n    def _derive_att_map(self, x):\n        '''\n        x           :(#bs, #node, #dim)\n        out_shape   :(#bs, #node, #node, 1)\n        '''\n        att_map = self._pairwise_mul_nodes(x)\n        # size: (#bs, #node, #node, #dim_out)\n        att_map = torch.tanh(self.att_proj(att_map))\n        # size: (#bs, #node, #node, 1)\n        att_map = torch.matmul(att_map, self.att_weight)\n\n        # apply temperature\n        att_map = att_map / self.temp\n\n        att_map = F.softmax(att_map, dim=-2)\n\n        return att_map\n\n    def _project(self, x, att_map):\n        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n        x2 = self.proj_without_att(x)\n\n        return x1 + x2\n\n    def _apply_BN(self, x):\n        org_size = x.size()\n        x = x.view(-1, org_size[-1])\n        x = self.bn(x)\n        x = x.view(org_size)\n\n        return x\n\n    def _init_new_params(self, *size):\n        out = nn.Parameter(torch.FloatTensor(*size))\n        nn.init.xavier_normal_(out)\n        return out\n\n\nclass HtrgGraphAttentionLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, **kwargs):\n        super().__init__()\n\n        self.proj_type1 = nn.Linear(in_dim, in_dim)\n        self.proj_type2 = nn.Linear(in_dim, in_dim)\n\n        # attention map\n        self.att_proj = nn.Linear(in_dim, out_dim)\n        self.att_projM = nn.Linear(in_dim, out_dim)\n\n        self.att_weight11 = self._init_new_params(out_dim, 1)\n        self.att_weight22 = self._init_new_params(out_dim, 1)\n        self.att_weight12 = self._init_new_params(out_dim, 1)\n        self.att_weightM = self._init_new_params(out_dim, 1)\n\n        # project\n        self.proj_with_att = nn.Linear(in_dim, out_dim)\n        self.proj_without_att = nn.Linear(in_dim, out_dim)\n\n        self.proj_with_attM = nn.Linear(in_dim, out_dim)\n        self.proj_without_attM = nn.Linear(in_dim, out_dim)\n\n        # batch norm\n        self.bn = nn.BatchNorm1d(out_dim)\n\n        # dropout for inputs\n        self.input_drop = nn.Dropout(p=0.2)\n\n        # activate\n        self.act = nn.SELU(inplace=True)\n\n        # temperature\n        self.temp = 1.\n        if \"temperature\" in kwargs:\n            self.temp = kwargs[\"temperature\"]\n\n    def forward(self, x1, x2, master=None):\n        '''\n        x1  :(#bs, #node, #dim)\n        x2  :(#bs, #node, #dim)\n        '''\n        num_type1 = x1.size(1)\n        num_type2 = x2.size(1)\n\n        x1 = self.proj_type1(x1)\n        x2 = self.proj_type2(x2)\n\n        x = torch.cat([x1, x2], dim=1)\n\n        if master is None:\n            master = torch.mean(x, dim=1, keepdim=True)\n\n        # apply input dropout\n        x = self.input_drop(x)\n\n        # derive attention map\n        att_map = self._derive_att_map(x, num_type1, num_type2)\n\n        # directional edge for master node\n        master = self._update_master(x, master)\n\n        # projection\n        x = self._project(x, att_map)\n\n        # apply batch norm\n        x = self._apply_BN(x)\n        x = self.act(x)\n\n        x1 = x.narrow(1, 0, num_type1)\n        x2 = x.narrow(1, num_type1, num_type2)\n\n        return x1, x2, master\n\n    def _update_master(self, x, master):\n\n        att_map = self._derive_att_map_master(x, master)\n        master = self._project_master(x, master, att_map)\n\n        return master\n\n    def _pairwise_mul_nodes(self, x):\n        '''\n        Calculates pairwise multiplication of nodes.\n        - for attention map\n        x           :(#bs, #node, #dim)\n        out_shape   :(#bs, #node, #node, #dim)\n        '''\n\n        nb_nodes = x.size(1)\n        x = x.unsqueeze(2).expand(-1, -1, nb_nodes, -1)\n        x_mirror = x.transpose(1, 2)\n\n        return x * x_mirror\n\n    def _derive_att_map_master(self, x, master):\n        '''\n        x           :(#bs, #node, #dim)\n        out_shape   :(#bs, #node, #node, 1)\n        '''\n        att_map = x * master\n        att_map = torch.tanh(self.att_projM(att_map))\n\n        att_map = torch.matmul(att_map, self.att_weightM)\n\n        # apply temperature\n        att_map = att_map / self.temp\n\n        att_map = F.softmax(att_map, dim=-2)\n\n        return att_map\n\n    def _derive_att_map(self, x, num_type1, num_type2):\n        '''\n        x           :(#bs, #node, #dim)\n        out_shape   :(#bs, #node, #node, 1)\n        '''\n        att_map = self._pairwise_mul_nodes(x)\n        # size: (#bs, #node, #node, #dim_out)\n        att_map = torch.tanh(self.att_proj(att_map))\n        # size: (#bs, #node, #node, 1)\n\n        att_board = torch.zeros_like(att_map[:, :, :, 0]).unsqueeze(-1)\n\n        att_board[:, :num_type1, :num_type1, :] = torch.matmul(\n            att_map[:, :num_type1, :num_type1, :], self.att_weight11)\n        att_board[:, num_type1:, num_type1:, :] = torch.matmul(\n            att_map[:, num_type1:, num_type1:, :], self.att_weight22)\n        att_board[:, :num_type1, num_type1:, :] = torch.matmul(\n            att_map[:, :num_type1, num_type1:, :], self.att_weight12)\n        att_board[:, num_type1:, :num_type1, :] = torch.matmul(\n            att_map[:, num_type1:, :num_type1, :], self.att_weight12)\n\n        att_map = att_board\n\n        # att_map = torch.matmul(att_map, self.att_weight12)\n\n        # apply temperature\n        att_map = att_map / self.temp\n\n        att_map = F.softmax(att_map, dim=-2)\n\n        return att_map\n\n    def _project(self, x, att_map):\n        x1 = self.proj_with_att(torch.matmul(att_map.squeeze(-1), x))\n        x2 = self.proj_without_att(x)\n\n        return x1 + x2\n\n    def _project_master(self, x, master, att_map):\n\n        x1 = self.proj_with_attM(torch.matmul(\n            att_map.squeeze(-1).unsqueeze(1), x))\n        x2 = self.proj_without_attM(master)\n\n        return x1 + x2\n\n    def _apply_BN(self, x):\n        org_size = x.size()\n        x = x.view(-1, org_size[-1])\n        x = self.bn(x)\n        x = x.view(org_size)\n\n        return x\n\n    def _init_new_params(self, *size):\n        out = nn.Parameter(torch.FloatTensor(*size))\n        nn.init.xavier_normal_(out)\n        return out\n\n\nclass GraphPool(nn.Module):\n    def __init__(self, k: float, in_dim: int, p: Union[float, int]):\n        super().__init__()\n        self.k = k\n        self.sigmoid = nn.Sigmoid()\n        self.proj = nn.Linear(in_dim, 1)\n        self.drop = nn.Dropout(p=p) if p > 0 else nn.Identity()\n        self.in_dim = in_dim\n\n    def forward(self, h):\n        Z = self.drop(h)\n        weights = self.proj(Z)\n        scores = self.sigmoid(weights)\n        new_h = self.top_k_graph(scores, h, self.k)\n\n        return new_h\n\n    def top_k_graph(self, scores, h, k):\n        \"\"\"\n        args\n        =====\n        scores: attention-based weights (#bs, #node, 1)\n        h: graph data (#bs, #node, #dim)\n        k: ratio of remaining nodes, (float)\n\n        returns\n        =====\n        h: graph pool applied data (#bs, #node', #dim)\n        \"\"\"\n        _, n_nodes, n_feat = h.size()\n        n_nodes = max(int(n_nodes * k), 1)\n        _, idx = torch.topk(scores, n_nodes, dim=1)\n        idx = idx.expand(-1, -1, n_feat)\n\n        h = h * scores\n        h = torch.gather(h, 1, idx)\n\n        return h\n\n\nclass CONV(nn.Module):\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10**(mel / 2595) - 1)\n\n    def __init__(self,\n                 out_channels,\n                 kernel_size,\n                 sample_rate=16000,\n                 in_channels=1,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 bias=False,\n                 groups=1,\n                 mask=False):\n        super().__init__()\n        if in_channels != 1:\n\n            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (\n                in_channels)\n            raise ValueError(msg)\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.sample_rate = sample_rate\n\n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size % 2 == 0:\n            self.kernel_size = self.kernel_size + 1\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n        self.mask = mask\n        if bias:\n            raise ValueError('SincConv does not support bias.')\n        if groups > 1:\n            raise ValueError('SincConv does not support groups.')\n\n        NFFT = 512\n        f = int(self.sample_rate / 2) * np.linspace(0, 1, int(NFFT / 2) + 1)\n        fmel = self.to_mel(f)\n        fmelmax = np.max(fmel)\n        fmelmin = np.min(fmel)\n        filbandwidthsmel = np.linspace(fmelmin, fmelmax, self.out_channels + 1)\n        filbandwidthsf = self.to_hz(filbandwidthsmel)\n\n        self.mel = filbandwidthsf\n        self.hsupp = torch.arange(-(self.kernel_size - 1) / 2,\n                                  (self.kernel_size - 1) / 2 + 1)\n        self.band_pass = torch.zeros(self.out_channels, self.kernel_size)\n        for i in range(len(self.mel) - 1):\n            fmin = self.mel[i]\n            fmax = self.mel[i + 1]\n            hHigh = (2*fmax/self.sample_rate) * \\\n                np.sinc(2*fmax*self.hsupp/self.sample_rate)\n            hLow = (2*fmin/self.sample_rate) * \\\n                np.sinc(2*fmin*self.hsupp/self.sample_rate)\n            hideal = hHigh - hLow\n\n            self.band_pass[i, :] = Tensor(np.hamming(\n                self.kernel_size)) * Tensor(hideal)\n\n    def forward(self, x, mask=False):\n        band_pass_filter = self.band_pass.clone().to(x.device)\n        if mask:\n            A = np.random.uniform(0, 20)\n            A = int(A)\n            A0 = random.randint(0, band_pass_filter.shape[0] - A)\n            band_pass_filter[A0:A0 + A, :] = 0\n        else:\n            band_pass_filter = band_pass_filter\n\n        self.filters = (band_pass_filter).view(self.out_channels, 1,\n                                               self.kernel_size)\n\n        return F.conv1d(x,\n                        self.filters,\n                        stride=self.stride,\n                        padding=self.padding,\n                        dilation=self.dilation,\n                        bias=None,\n                        groups=1)\n\n\nclass Residual_block(nn.Module):\n    def __init__(self, nb_filts, first=False):\n        super().__init__()\n        self.first = first\n\n        if not self.first:\n            self.bn1 = nn.BatchNorm2d(num_features=nb_filts[0])\n        self.conv1 = nn.Conv2d(in_channels=nb_filts[0],\n                               out_channels=nb_filts[1],\n                               kernel_size=(2, 3),\n                               padding=(1, 1),\n                               stride=1)\n        self.selu = nn.SELU(inplace=True)\n\n        self.bn2 = nn.BatchNorm2d(num_features=nb_filts[1])\n        self.conv2 = nn.Conv2d(in_channels=nb_filts[1],\n                               out_channels=nb_filts[1],\n                               kernel_size=(2, 3),\n                               padding=(0, 1),\n                               stride=1)\n\n        if nb_filts[0] != nb_filts[1]:\n            self.downsample = True\n            self.conv_downsample = nn.Conv2d(in_channels=nb_filts[0],\n                                             out_channels=nb_filts[1],\n                                             padding=(0, 1),\n                                             kernel_size=(1, 3),\n                                             stride=1)\n\n        else:\n            self.downsample = False\n        self.mp = nn.MaxPool2d((1, 3))  # self.mp = nn.MaxPool2d((1,4))\n\n    def forward(self, x):\n        identity = x\n        if not self.first:\n            out = self.bn1(x)\n            out = self.selu(out)\n        else:\n            out = x\n        out = self.conv1(x)\n\n        # print('out',out.shape)\n        out = self.bn2(out)\n        out = self.selu(out)\n        # print('out',out.shape)\n        out = self.conv2(out)\n        #print('conv2 out',out.shape)\n        if self.downsample:\n            identity = self.conv_downsample(identity)\n\n        out += identity\n        out = self.mp(out)\n        return out\n\n\nclass AASIST_Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        d_args = {\n        \"architecture\": \"AASIST\",\n        \"nb_samp\": 64600,\n        \"first_conv\": 128,\n        \"filts\": [70, [1, 32], [32, 32], [32, 64], [64, 64]],\n        \"gat_dims\": [64, 32],\n        \"pool_ratios\": [0.5, 0.7, 0.5, 0.5],\n        \"temperatures\": [2.0, 2.0, 100.0, 100.0]\n        }\n\n        self.d_args = d_args\n        filts = d_args[\"filts\"]\n        gat_dims = d_args[\"gat_dims\"]\n        pool_ratios = d_args[\"pool_ratios\"]\n        temperatures = d_args[\"temperatures\"]\n\n        self.conv_time = CONV(out_channels=filts[0],\n                              kernel_size=d_args[\"first_conv\"],\n                              in_channels=1)\n        self.first_bn = nn.BatchNorm2d(num_features=1)\n\n        self.drop = nn.Dropout(0.5, inplace=True)\n        self.drop_way = nn.Dropout(0.2, inplace=True)\n        self.selu = nn.SELU(inplace=True)\n\n        self.encoder = nn.Sequential(\n            nn.Sequential(Residual_block(nb_filts=filts[1], first=True)),\n            nn.Sequential(Residual_block(nb_filts=filts[2])),\n            nn.Sequential(Residual_block(nb_filts=filts[3])),\n            nn.Sequential(Residual_block(nb_filts=filts[4])),\n            nn.Sequential(Residual_block(nb_filts=filts[4])),\n            nn.Sequential(Residual_block(nb_filts=filts[4])))\n\n        self.pos_S = nn.Parameter(torch.randn(1, 23, filts[-1][-1]))\n        self.master1 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n        self.master2 = nn.Parameter(torch.randn(1, 1, gat_dims[0]))\n\n        self.GAT_layer_S = GraphAttentionLayer(filts[-1][-1],\n                                               gat_dims[0],\n                                               temperature=temperatures[0])\n        self.GAT_layer_T = GraphAttentionLayer(filts[-1][-1],\n                                               gat_dims[0],\n                                               temperature=temperatures[1])\n\n        self.HtrgGAT_layer_ST11 = HtrgGraphAttentionLayer(\n            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n        self.HtrgGAT_layer_ST12 = HtrgGraphAttentionLayer(\n            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n\n        self.HtrgGAT_layer_ST21 = HtrgGraphAttentionLayer(\n            gat_dims[0], gat_dims[1], temperature=temperatures[2])\n\n        self.HtrgGAT_layer_ST22 = HtrgGraphAttentionLayer(\n            gat_dims[1], gat_dims[1], temperature=temperatures[2])\n\n        self.pool_S = GraphPool(pool_ratios[0], gat_dims[0], 0.3)\n        self.pool_T = GraphPool(pool_ratios[1], gat_dims[0], 0.3)\n        self.pool_hS1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n        self.pool_hT1 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n\n        self.pool_hS2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n        self.pool_hT2 = GraphPool(pool_ratios[2], gat_dims[1], 0.3)\n\n        self.out_layer = nn.Linear(5 * gat_dims[1], 2)\n\n    def forward(self, x, Freq_aug=False):\n\n        x = x.unsqueeze(1)\n        x = self.conv_time(x, mask=Freq_aug)\n        x = x.unsqueeze(dim=1)\n        x = F.max_pool2d(torch.abs(x), (3, 3))\n        x = self.first_bn(x)\n        x = self.selu(x)\n\n        # get embeddings using encoder\n        # (#bs, #filt, #spec, #seq)\n        e = self.encoder(x)\n\n        # spectral GAT (GAT-S)\n        e_S, _ = torch.max(torch.abs(e), dim=3)  # max along time\n        e_S = e_S.transpose(1, 2) + self.pos_S\n\n        gat_S = self.GAT_layer_S(e_S)\n        out_S = self.pool_S(gat_S)  # (#bs, #node, #dim)\n\n        # temporal GAT (GAT-T)\n        e_T, _ = torch.max(torch.abs(e), dim=2)  # max along freq\n        e_T = e_T.transpose(1, 2)\n\n        gat_T = self.GAT_layer_T(e_T)\n        out_T = self.pool_T(gat_T)\n\n        # learnable master node\n        master1 = self.master1.expand(x.size(0), -1, -1)\n        master2 = self.master2.expand(x.size(0), -1, -1)\n\n        # inference 1\n        out_T1, out_S1, master1 = self.HtrgGAT_layer_ST11(\n            out_T, out_S, master=self.master1)\n\n        out_S1 = self.pool_hS1(out_S1)\n        out_T1 = self.pool_hT1(out_T1)\n\n        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST12(\n            out_T1, out_S1, master=master1)\n        out_T1 = out_T1 + out_T_aug\n        out_S1 = out_S1 + out_S_aug\n        master1 = master1 + master_aug\n\n        # inference 2\n        out_T2, out_S2, master2 = self.HtrgGAT_layer_ST21(\n            out_T, out_S, master=self.master2)\n        out_S2 = self.pool_hS2(out_S2)\n        out_T2 = self.pool_hT2(out_T2)\n\n        out_T_aug, out_S_aug, master_aug = self.HtrgGAT_layer_ST22(\n            out_T2, out_S2, master=master2)\n        out_T2 = out_T2 + out_T_aug\n        out_S2 = out_S2 + out_S_aug\n        master2 = master2 + master_aug\n\n        out_T1 = self.drop_way(out_T1)\n        out_T2 = self.drop_way(out_T2)\n        out_S1 = self.drop_way(out_S1)\n        out_S2 = self.drop_way(out_S2)\n        master1 = self.drop_way(master1)\n        master2 = self.drop_way(master2)\n\n        out_T = torch.max(out_T1, out_T2)\n        out_S = torch.max(out_S1, out_S2)\n        master = torch.max(master1, master2)\n\n        T_max, _ = torch.max(torch.abs(out_T), dim=1)\n        T_avg = torch.mean(out_T, dim=1)\n\n        S_max, _ = torch.max(torch.abs(out_S), dim=1)\n        S_avg = torch.mean(out_S, dim=1)\n\n        last_hidden = torch.cat(\n            [T_max, T_avg, S_max, S_avg, master.squeeze(1)], dim=1)\n\n        last_hidden = self.drop(last_hidden)\n        output = self.out_layer(last_hidden)\n\n        return last_hidden, output\n\n    def pad(self, x, max_len=64600): # 64600 samples = 4 giÃ¢y\n      x_len = x.shape[0]\n      if x_len >= max_len:\n          return x[:max_len]\n      # need to pad\n      num_repeats = int(max_len / x_len) + 1\n      padded_x = np.tile(x, (1, num_repeats))[:, :max_len][0]\n      return padded_x\n\n    def getScore(self, path_file_test: Path | str):\n      x, _ = soundfile.read(path_file_test)\n      x_pad = self.pad(x)\n      x_inp = Tensor(x_pad)\n      x_inp = torch.unsqueeze(x_inp, 0).to(device)\n      self.eval()\n      with torch.no_grad():\n            last_hidden, output = self.forward(x_inp)\n      scores = F.softmax(output) ### cÃ³ 2 score ??? láº¥y tháº¿ nÃ o --> láº¥y cÃ¡i Ä‘iá»ƒm Ä‘Ã¡nh giÃ¡ x khÃ´ng pháº£i spoof, lÃ  real\n\n#       print(scores[0])\n      return scores[0][1].detach().cpu().numpy()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.202469Z","iopub.execute_input":"2025-08-03T04:13:25.203234Z","iopub.status.idle":"2025-08-03T04:13:25.256141Z","shell.execute_reply.started":"2025-08-03T04:13:25.203212Z","shell.execute_reply":"2025-08-03T04:13:25.255377Z"}},"outputs":[],"execution_count":57},{"cell_type":"markdown","source":"# Compute eer","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\nimport numpy as np\n\ndef compute_eer(y_true, y_score):\n    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label=1)\n    fnr = 1 - tpr\n    eer_threshold = thresholds[np.nanargmin(np.absolute((fnr - fpr)))]\n    eer = fpr[np.nanargmin(np.absolute((fnr - fpr)))]\n    return eer, eer_threshold\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.257193Z","iopub.execute_input":"2025-08-03T04:13:25.257479Z","iopub.status.idle":"2025-08-03T04:13:25.277496Z","shell.execute_reply.started":"2025-08-03T04:13:25.257444Z","shell.execute_reply":"2025-08-03T04:13:25.276692Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"# Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport soundfile as sf\nimport os\nimport numpy as np\nfrom collections import defaultdict\nimport random\n\nclass AASISTDataset(Dataset):\n    def __init__(self, root_dir, list_file, max_len=64600, eval_mode=False):\n        \"\"\"\n        root_dir: thÆ° má»¥c gá»‘c\n        list_file: file txt chá»©a list wav + nhÃ£n\n        max_len: sá»‘ sample (4s @16kHz = 64000, mÃ¬nh Ä‘á»ƒ 64600 cho cháº¯c)\n        eval_mode: náº¿u True thÃ¬ crop á»Ÿ giá»¯a (deterministic), dÃ¹ng cho validation/test\n        \"\"\"\n        self.root_dir = root_dir\n        self.max_len = max_len\n        self.eval_mode = eval_mode\n        self.data = []\n        self.speaker_to_indices = defaultdict(lambda: {\"bonafide\": [], \"spoof\": []})\n\n        with open(list_file, \"r\") as f:\n            for idx, line in enumerate(f):\n                path, label_str = line.strip().split()\n                label = 0 if label_str == \"bonafide\" else 1\n                self.data.append((path, label))\n\n                # láº¥y speaker id\n                speaker_id = path.split(\"/\")[0]  \n                if label == 0:\n                    self.speaker_to_indices[speaker_id][\"bonafide\"].append(idx)\n                else:\n                    self.speaker_to_indices[speaker_id][\"spoof\"].append(idx)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        rel_path, label = self.data[idx]\n        full_path = os.path.join(self.root_dir, rel_path)\n        waveform, _ = sf.read(full_path)\n\n        L = len(waveform)\n\n        if L > self.max_len:\n            if self.eval_mode:\n                # crop á»Ÿ giá»¯a cho validation/test\n                start = (L - self.max_len) // 2\n            else:\n                # random crop cho training\n                start = random.randint(0, L - self.max_len)\n            waveform = waveform[start:start + self.max_len]\n\n        elif L < self.max_len:\n            # pad cho Ä‘á»§\n            pad = self.max_len - L\n            waveform = np.pad(waveform, (0, pad))\n\n        waveform_tensor = torch.FloatTensor(waveform)\n        return waveform_tensor, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.278269Z","iopub.execute_input":"2025-08-03T04:13:25.278483Z","iopub.status.idle":"2025-08-03T04:13:25.289737Z","shell.execute_reply.started":"2025-08-03T04:13:25.278466Z","shell.execute_reply":"2025-08-03T04:13:25.289125Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"class AASISTValDataset(Dataset):\n    def __init__(self, root_dir, list_file, max_len=64600):\n        \"\"\"\n        Dataset cho VALIDATION/TEST\n        - Center crop 4s (deterministic)\n        \"\"\"\n        self.root_dir = root_dir\n        self.max_len = max_len\n        self.data = []\n\n        with open(list_file, \"r\") as f:\n            for line in f:\n                path, label_str = line.strip().split()\n                label = 0 if label_str == \"bonafide\" else 1\n                self.data.append((path, label))\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        rel_path, label = self.data[idx]\n        full_path = os.path.join(self.root_dir, rel_path)\n\n        waveform, _ = sf.read(full_path)\n        L = len(waveform)\n\n        if L > self.max_len:\n            # center crop\n            start = (L - self.max_len) // 2\n            waveform = waveform[start:start + self.max_len]\n        elif L < self.max_len:\n            # pad\n            pad = self.max_len - L\n            waveform = np.pad(waveform, (0, pad))\n\n        waveform_tensor = torch.FloatTensor(waveform)\n        return waveform_tensor, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.291868Z","iopub.execute_input":"2025-08-03T04:13:25.292155Z","iopub.status.idle":"2025-08-03T04:13:25.311121Z","shell.execute_reply.started":"2025-08-03T04:13:25.292131Z","shell.execute_reply":"2025-08-03T04:13:25.310356Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"root_dir = \"/kaggle/input/vsasv-augment/vlsp2025/vlsp2025/train\"\ntrain_path = \"/kaggle/input/cm-training/train_list.txt\"\nval_path = \"/kaggle/input/cm-training/val_list.txt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.311943Z","iopub.execute_input":"2025-08-03T04:13:25.312218Z","iopub.status.idle":"2025-08-03T04:13:25.325996Z","shell.execute_reply.started":"2025-08-03T04:13:25.312194Z","shell.execute_reply":"2025-08-03T04:13:25.325426Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"# Batch Sampler","metadata":{}},{"cell_type":"code","source":"import random\nfrom torch.utils.data import Sampler\n\nclass SpeakerPairBatchSampler(Sampler):\n    def __init__(self, dataset, speakers_per_batch=16, seed=42, infinite=True):\n        self.dataset = dataset\n        self.speakers = list(dataset.speaker_to_indices.keys())\n        self.speakers_per_batch = speakers_per_batch\n        self.seed = seed\n        self.infinite = infinite\n        random.seed(seed)\n\n    def __iter__(self):\n        while True:\n            selected_speakers = random.sample(self.speakers, self.speakers_per_batch)\n            batch_indices = []\n            for spk in selected_speakers:\n                bonafide_list = self.dataset.speaker_to_indices[spk][\"bonafide\"]\n                spoof_list = self.dataset.speaker_to_indices[spk][\"spoof\"]\n\n                if len(bonafide_list) == 0 or len(spoof_list) == 0:\n                    continue  # skip náº¿u speaker thiáº¿u loáº¡i nÃ o\n\n                b_idx = random.choice(bonafide_list)\n                s_idx = random.choice(spoof_list)\n                batch_indices.extend([b_idx, s_idx])\n\n            yield batch_indices\n            if not self.infinite:\n                break\n\n    def __len__(self):\n        # KhÃ´ng xÃ¡c Ä‘á»‹nh náº¿u infinite\n        return len(self.speakers) // self.speakers_per_batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.326698Z","iopub.execute_input":"2025-08-03T04:13:25.326960Z","iopub.status.idle":"2025-08-03T04:13:25.342160Z","shell.execute_reply.started":"2025-08-03T04:13:25.326943Z","shell.execute_reply":"2025-08-03T04:13:25.341378Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Train dataset: random crop (eval_mode=False)\ntrain_dataset = AASISTDataset(\n    root_dir=root_dir,\n    list_file=train_path,\n    eval_mode=False\n)\n\n# Val dataset: center crop (eval_mode=True)\nval_dataset = AASISTValDataset(\n    root_dir=root_dir,\n    list_file=val_path,\n)\n\n# Sampler cho train (16 speaker â†’ 32 file má»—i batch)\ntrain_sampler = SpeakerPairBatchSampler(\n    dataset=train_dataset,\n    speakers_per_batch=16,\n    infinite=True  # cho train thÃ¬ láº·p vÃ´ háº¡n\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.342819Z","iopub.execute_input":"2025-08-03T04:13:25.343032Z","iopub.status.idle":"2025-08-03T04:13:25.407198Z","shell.execute_reply.started":"2025-08-03T04:13:25.343008Z","shell.execute_reply":"2025-08-03T04:13:25.406642Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"train_loader = DataLoader(\n    train_dataset,\n    batch_sampler=train_sampler,\n    num_workers=4,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    num_workers=4,\n    pin_memory=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.407959Z","iopub.execute_input":"2025-08-03T04:13:25.408239Z","iopub.status.idle":"2025-08-03T04:13:25.414442Z","shell.execute_reply.started":"2025-08-03T04:13:25.408218Z","shell.execute_reply":"2025-08-03T04:13:25.413619Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"# Train function","metadata":{}},{"cell_type":"code","source":"import itertools\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom tqdm import tqdm\n\ndef train(model,\n          train_loader: DataLoader,\n          val_loader: DataLoader,\n          num_epochs: int = 30,\n          steps_per_epoch: int = 2000,   # ðŸ‘ˆ sá»‘ step má»—i epoch\n          lr: float = 1e-4,\n          save_path: str = \"./best_model.pth\",\n          global_step = 0,\n          device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n\n    # Khá»Ÿi táº¡o wandb\n    wandb.init(project=\"aasist\", config={\n        \"epochs\": num_epochs,\n        \"steps_per_epoch\": steps_per_epoch,\n        \"learning_rate\": lr,\n        \"optimizer\": \"AdamW\",\n        \"scheduler\": \"StepLR\",\n        \"architecture\": \"AASIST\",\n    })\n\n    model = model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    best_eer = 1.0\n\n    for epoch in range(1, num_epochs + 1):\n        model.train()\n        train_loss = 0.0\n        correct = 0\n        total = 0\n\n        # ðŸ‘‡ láº¥y iterator vÃ´ háº¡n tá»« train_loader\n        infinite_loader = itertools.cycle(train_loader)\n        loop = tqdm(range(steps_per_epoch), desc=f\"[Epoch {epoch}/{num_epochs}] Training\", leave=False)\n\n        for _ in loop:\n            x, y = next(infinite_loader)\n            x, y = x.to(device), y.to(device)\n\n            optimizer.zero_grad()\n            _, output = model(x)\n\n            loss = criterion(output, y)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item() * x.size(0)\n            preds = torch.argmax(output, dim=1)\n            correct += (preds == y).sum().item()\n            total += x.size(0)\n\n            loop.set_postfix(loss=loss.item())\n            if global_step % 500 == 0 and global_step != 0:\n                wandb.log({\n                    \"train/loss_step\": loss.item(),\n                    \"train/lr\": scheduler.get_last_lr()[0],\n                    \"step\": global_step\n                })\n\n            global_step += 1\n\n        train_acc = correct / total\n        avg_train_loss = train_loss / total\n\n        # Validation\n        model.eval()\n        all_labels, all_scores = [], []\n\n        val_loop = tqdm(val_loader, desc=f\"[Epoch {epoch}/{num_epochs}] Validation\", leave=False)\n\n        with torch.no_grad():\n            for x, y in val_loop:\n                x = x.to(device)\n                _, output = model(x)\n                scores = F.softmax(output, dim=1)[:, 1]  # bonafide score\n                all_labels.extend(y.cpu().numpy())\n                all_scores.extend(scores.cpu().numpy())\n\n        eer, eer_threshold = compute_eer(np.array(all_labels), np.array(all_scores))\n\n        print(f\"[Epoch {epoch}] Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.4f} | Val EER: {eer:.4f}\")\n\n        wandb.log({\n            \"epoch\": epoch,\n            \"train_loss\": avg_train_loss,\n            \"train_acc\": train_acc,\n            \"val_eer\": eer,\n            \"lr\": scheduler.get_last_lr()[0]\n        })\n\n        if eer < best_eer:\n            best_eer = eer\n            torch.save(model.state_dict(), save_path)\n            print(f\"âœ… New best model saved at epoch {epoch} with EER={eer:.4f}\")\n\n        scheduler.step()\n\n    print(\"ðŸŽ‰ Finished Training.\")\n    wandb.finish()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:16:26.865806Z","iopub.execute_input":"2025-08-03T04:16:26.866147Z","iopub.status.idle":"2025-08-03T04:16:26.879307Z","shell.execute_reply.started":"2025-08-03T04:16:26.866118Z","shell.execute_reply":"2025-08-03T04:16:26.878481Z"}},"outputs":[],"execution_count":69},{"cell_type":"code","source":"!pip install wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:25.437542Z","iopub.execute_input":"2025-08-03T04:13:25.437738Z","iopub.status.idle":"2025-08-03T04:13:28.637788Z","shell.execute_reply.started":"2025-08-03T04:13:25.437724Z","shell.execute_reply":"2025-08-03T04:13:28.637132Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.20.1)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.2.1)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from wandb) (25.0)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.8)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (7.0.0)\nRequirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.7)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.4)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.31.0)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\nRequirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.14.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.6.15)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"import os, wandb\n\nos.environ[\"WANDB_KEY\"] = \"4b8af864ea6d5ec9af172b42a4c40e4444e20cf7\"\nwandb.login(key=os.getenv(\"WANDB_KEY\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:13:28.638807Z","iopub.execute_input":"2025-08-03T04:13:28.639116Z","iopub.status.idle":"2025-08-03T04:13:28.648055Z","shell.execute_reply.started":"2025-08-03T04:13:28.639065Z","shell.execute_reply":"2025-08-03T04:13:28.647392Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"model = AASIST_Model()\ntrain(model, train_loader, val_loader, num_epochs=50)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-03T04:16:30.723921Z","iopub.execute_input":"2025-08-03T04:16:30.724687Z","iopub.status.idle":"2025-08-03T04:16:51.736323Z","shell.execute_reply.started":"2025-08-03T04:16:30.724662Z","shell.execute_reply":"2025-08-03T04:16:51.735239Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing previous runs because reinit is set to 'default'."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">balmy-firefly-23</strong> at: <a href='https://wandb.ai/hddat2k4-uit/aasist/runs/lzt31iny' target=\"_blank\">https://wandb.ai/hddat2k4-uit/aasist/runs/lzt31iny</a><br> View project at: <a href='https://wandb.ai/hddat2k4-uit/aasist' target=\"_blank\">https://wandb.ai/hddat2k4-uit/aasist</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250803_041328-lzt31iny/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250803_041630-c3bg2h2t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hddat2k4-uit/aasist/runs/c3bg2h2t' target=\"_blank\">worthy-resonance-24</a></strong> to <a href='https://wandb.ai/hddat2k4-uit/aasist' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hddat2k4-uit/aasist' target=\"_blank\">https://wandb.ai/hddat2k4-uit/aasist</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hddat2k4-uit/aasist/runs/c3bg2h2t' target=\"_blank\">https://wandb.ai/hddat2k4-uit/aasist/runs/c3bg2h2t</a>"},"metadata":{}},{"name":"stderr","text":"                                                                                    \r","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2837981028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAASIST_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/2700821485.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, steps_per_epoch, lr, save_path, global_step, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n","output_type":"stream"}],"execution_count":70}]}